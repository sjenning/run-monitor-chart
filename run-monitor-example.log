Mar 04 20:37:52.943 E kube-apiserver failed contacting the API: Get https://api.sjenning.devcluster.openshift.com:6443/apis/config.openshift.io/v1/clusterversions?fieldSelector=metadata.name%3Dversion&limit=500&resourceVersion=0: dial tcp 54.176.20.29:6443: connect: connection refused
Mar 04 20:37:53.094 E kube-apiserver failed contacting the API: the server could not find the requested resource (get clusteroperators.config.openshift.io)
Mar 04 20:37:53.897 I openshift-apiserver OpenShift API started failing: the server could not find the requested resource (get imagestreams.image.openshift.io missing)
Mar 04 20:38:00.394 I clusterversion/version created
Mar 04 20:38:06.993 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1"
Mar 04 20:38:07.012 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (2 times)
Mar 04 20:38:07.035 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (3 times)
Mar 04 20:38:07.060 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (4 times)
Mar 04 20:38:07.080 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (5 times)
Mar 04 20:38:07.108 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (6 times)
Mar 04 20:38:07.277 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (7 times)
Mar 04 20:38:07.608 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (8 times)
Mar 04 20:38:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:38:08.259 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (9 times)
Mar 04 20:38:09.551 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (10 times)
Mar 04 20:38:12.123 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (11 times)
Mar 04 20:38:17.263 W ns/openshift-cluster-version deployment/cluster-version-operator Failed to create new replica set "cluster-version-operator-5dcf9fb898": replicasets.apps "cluster-version-operator-5dcf9fb898" is forbidden: cannot set blockOwnerDeletion in this case because cannot find RESTMapping for APIVersion apps/v1 Kind Deployment: no matches for kind "Deployment" in version "apps/v1" (12 times)
Mar 04 20:38:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:38:27.528 I ns/openshift-cluster-version deployment/cluster-version-operator Scaled up replica set cluster-version-operator-5dcf9fb898 to 1
Mar 04 20:38:27.548 I ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs node/ created
Mar 04 20:38:27.556 W ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs no nodes available to schedule pods
Mar 04 20:38:27.557 I ns/openshift-cluster-version replicaset/cluster-version-operator-5dcf9fb898 Created pod: cluster-version-operator-5dcf9fb898-9jrhs
Mar 04 20:38:27.572 W ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs no nodes available to schedule pods (2 times)
Mar 04 20:38:31.354 I ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs Successfully assigned openshift-cluster-version/cluster-version-operator-5dcf9fb898-9jrhs to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:38:31.861 I ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs pulling image "registry.svc.ci.openshift.org/openshift/origin-release@sha256:933df182b35b1dc179bd0cfba6d3c0e0a15451989e52e950368c92cbd9e38cf2"
Mar 04 20:38:36.877 I ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-release@sha256:933df182b35b1dc179bd0cfba6d3c0e0a15451989e52e950368c92cbd9e38cf2"
Mar 04 20:38:37.043 I ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs Created container
Mar 04 20:38:37.063 I ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs Started container
Mar 04 20:38:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:38:37.852 W node/ip-10-0-133-9.us-west-1.compute.internal node is not ready
Mar 04 20:38:37.852 W node/ip-10-0-156-96.us-west-1.compute.internal node is not ready
Mar 04 20:38:41.376 I ns/openshift-cluster-version configmap/version ip-10-0-0-45_826f78c6-5d92-49d5-9d9d-e58394174034 became leader
Mar 04 20:38:41.478 W clusterversion/version cluster converging to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:38:43.945 I ns/openshift-network-operator deployment/network-operator Scaled up replica set network-operator-78688d7758 to 1
Mar 04 20:38:43.952 I ns/openshift-network-operator pod/network-operator-78688d7758-smzk4 node/ created
Mar 04 20:38:43.970 I ns/openshift-network-operator replicaset/network-operator-78688d7758 Created pod: network-operator-78688d7758-smzk4
Mar 04 20:38:43.979 I ns/openshift-network-operator pod/network-operator-78688d7758-smzk4 Successfully assigned openshift-network-operator/network-operator-78688d7758-smzk4 to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:38:44.475 I ns/openshift-network-operator pod/network-operator-78688d7758-smzk4 pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:7e005b9742fdeeae373e0c4d79cbab35da2385f4bfb183bd7205ca234cea5138"
Mar 04 20:38:50.346 I ns/openshift-network-operator pod/network-operator-78688d7758-smzk4 Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:7e005b9742fdeeae373e0c4d79cbab35da2385f4bfb183bd7205ca234cea5138"
Mar 04 20:38:50.508 I ns/openshift-network-operator pod/network-operator-78688d7758-smzk4 Created container
Mar 04 20:38:50.525 I ns/openshift-network-operator pod/network-operator-78688d7758-smzk4 Started container
Mar 04 20:38:50.995 I clusteroperator/network created
Mar 04 20:38:52.351 I ns/openshift-multus pod/multus-c765r node/ created
Mar 04 20:38:52.394 I ns/openshift-multus daemonset/multus Created pod: multus-c765r
Mar 04 20:38:52.394 I ns/openshift-multus pod/multus-c765r Successfully assigned openshift-multus/multus-c765r to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:38:52.394 E clusteroperator/network changed Failing to True: NoNamespace: Namespace "openshift-sdn" does not exist
Mar 04 20:38:52.394 I ns/openshift-multus pod/multus-pv6fw node/ created
Mar 04 20:38:52.394 I ns/openshift-multus daemonset/multus Created pod: multus-pv6fw
Mar 04 20:38:52.404 I ns/openshift-multus pod/multus-pv6fw Successfully assigned openshift-multus/multus-pv6fw to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:38:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:38:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:38:52.852 W node/ip-10-0-133-9.us-west-1.compute.internal node is not ready
Mar 04 20:38:52.852 W node/ip-10-0-156-96.us-west-1.compute.internal node is not ready
Mar 04 20:38:52.876 I ns/openshift-multus pod/multus-c765r pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a58730174dabd295551c090ac657b0f7e5d3c93a732c7a57015d8a8d9040e55e"
Mar 04 20:38:52.886 I ns/openshift-multus pod/multus-pv6fw pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a58730174dabd295551c090ac657b0f7e5d3c93a732c7a57015d8a8d9040e55e"
Mar 04 20:38:55.441 I ns/kube-system pod/etcd-member-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:38:57.325 I ns/openshift-multus pod/multus-pv6fw Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a58730174dabd295551c090ac657b0f7e5d3c93a732c7a57015d8a8d9040e55e"
Mar 04 20:38:57.350 I ns/openshift-multus pod/multus-c765r Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a58730174dabd295551c090ac657b0f7e5d3c93a732c7a57015d8a8d9040e55e"
Mar 04 20:38:57.489 I ns/openshift-multus pod/multus-pv6fw Created container
Mar 04 20:38:57.509 I ns/openshift-multus pod/multus-c765r Created container
Mar 04 20:38:57.512 I ns/openshift-multus pod/multus-pv6fw Started container
Mar 04 20:38:57.536 I ns/openshift-multus pod/multus-c765r Started container
Mar 04 20:38:57.735 I ns/openshift-multus pod/multus-pv6fw pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a892d084c92d90d10d2829e1dd7c0148f95893ff5f4749e9f50c313e2cdb040d"
Mar 04 20:38:57.807 I ns/openshift-multus pod/multus-c765r pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a892d084c92d90d10d2829e1dd7c0148f95893ff5f4749e9f50c313e2cdb040d"
Mar 04 20:38:59.389 I ns/openshift-multus pod/multus-5p7wr node/ created
Mar 04 20:38:59.399 I ns/openshift-multus daemonset/multus Created pod: multus-5p7wr
Mar 04 20:38:59.423 I ns/openshift-multus pod/multus-5p7wr Successfully assigned openshift-multus/multus-5p7wr to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:38:59.922 I ns/openshift-multus pod/multus-5p7wr pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a58730174dabd295551c090ac657b0f7e5d3c93a732c7a57015d8a8d9040e55e"
Mar 04 20:39:00.230 I ns/openshift-sdn pod/sdn-controller-gf8rh node/ created
Mar 04 20:39:00.239 I ns/openshift-sdn pod/sdn-controller-p66zk node/ created
Mar 04 20:39:00.240 I ns/openshift-sdn daemonset/sdn-controller Created pod: sdn-controller-gf8rh
Mar 04 20:39:00.273 I ns/openshift-sdn pod/sdn-controller-gf8rh Successfully assigned openshift-sdn/sdn-controller-gf8rh to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:39:00.274 I ns/openshift-sdn pod/sdn-controller-7s46m node/ created
Mar 04 20:39:00.274 I ns/openshift-sdn daemonset/sdn-controller Created pod: sdn-controller-p66zk
Mar 04 20:39:00.283 I ns/openshift-sdn pod/sdn-controller-p66zk Successfully assigned openshift-sdn/sdn-controller-p66zk to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:39:00.286 I ns/openshift-sdn daemonset/sdn-controller Created pod: sdn-controller-7s46m
Mar 04 20:39:00.287 I ns/openshift-sdn pod/sdn-controller-7s46m Successfully assigned openshift-sdn/sdn-controller-7s46m to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:39:00.767 I ns/openshift-sdn pod/sdn-controller-p66zk pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58"
Mar 04 20:39:00.777 I ns/openshift-sdn pod/sdn-controller-7s46m pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58"
Mar 04 20:39:01.664 I ns/openshift-sdn pod/sdn-controller-gf8rh pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58"
Mar 04 20:39:01.824 I ns/openshift-sdn pod/ovs-9mj4c node/ created
Mar 04 20:39:01.836 I ns/openshift-sdn daemonset/ovs Created pod: ovs-9mj4c
Mar 04 20:39:01.868 I ns/openshift-sdn pod/ovs-8gj4v node/ created
Mar 04 20:39:01.868 I ns/openshift-sdn pod/ovs-9mj4c Successfully assigned openshift-sdn/ovs-9mj4c to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:39:01.869 I ns/openshift-sdn pod/ovs-cmzgh node/ created
Mar 04 20:39:01.884 I ns/openshift-sdn daemonset/ovs Created pod: ovs-cmzgh
Mar 04 20:39:01.884 I ns/openshift-sdn pod/ovs-8gj4v Successfully assigned openshift-sdn/ovs-8gj4v to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:39:01.884 I ns/openshift-sdn daemonset/ovs Created pod: ovs-8gj4v
Mar 04 20:39:01.911 I ns/openshift-sdn pod/ovs-cmzgh Successfully assigned openshift-sdn/ovs-cmzgh to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:39:02.124 I ns/openshift-multus pod/multus-pv6fw Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a892d084c92d90d10d2829e1dd7c0148f95893ff5f4749e9f50c313e2cdb040d"
Mar 04 20:39:02.158 I ns/openshift-multus pod/multus-c765r Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a892d084c92d90d10d2829e1dd7c0148f95893ff5f4749e9f50c313e2cdb040d"
Mar 04 20:39:02.301 I ns/openshift-multus pod/multus-pv6fw Created container
Mar 04 20:39:02.331 I ns/openshift-multus pod/multus-pv6fw Started container
Mar 04 20:39:02.361 I ns/openshift-sdn pod/ovs-8gj4v pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:02.363 I ns/openshift-multus pod/multus-c765r Created container
Mar 04 20:39:02.389 I ns/openshift-multus pod/multus-c765r Started container
Mar 04 20:39:02.395 I ns/openshift-sdn pod/ovs-cmzgh pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:02.408 I ns/openshift-sdn pod/ovs-9mj4c pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:02.612 W clusteroperator/network changed Failing to False
Mar 04 20:39:02.612 W clusteroperator/network changed Progressing to True: Deploying: DaemonSet "openshift-multus/multus" is not available (awaiting 3 nodes)\nDaemonSet "openshift-sdn/sdn-controller" is not available (awaiting 3 nodes)\nDaemonSet "openshift-sdn/ovs" is not available (awaiting 3 nodes)\nDaemonSet "openshift-sdn/sdn" is not yet scheduled on any nodes
Mar 04 20:39:02.635 I ns/openshift-sdn pod/sdn-97nhs node/ created
Mar 04 20:39:02.641 I ns/openshift-sdn daemonset/sdn Created pod: sdn-97nhs
Mar 04 20:39:02.647 I ns/openshift-sdn pod/sdn-7x2ns node/ created
Mar 04 20:39:02.651 I ns/openshift-sdn pod/sdn-kq466 node/ created
Mar 04 20:39:02.653 I ns/openshift-sdn pod/sdn-97nhs Successfully assigned openshift-sdn/sdn-97nhs to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:39:02.663 I ns/openshift-sdn daemonset/sdn Created pod: sdn-kq466
Mar 04 20:39:02.680 I ns/openshift-sdn daemonset/sdn Created pod: sdn-7x2ns
Mar 04 20:39:02.683 I ns/openshift-sdn pod/sdn-7x2ns Successfully assigned openshift-sdn/sdn-7x2ns to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:39:02.693 I ns/openshift-sdn pod/sdn-kq466 Successfully assigned openshift-sdn/sdn-kq466 to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:39:02.745 I ns/openshift-multus pod/multus-pv6fw pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d"
Mar 04 20:39:02.815 I ns/openshift-multus pod/multus-c765r pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d"
Mar 04 20:39:03.138 I ns/openshift-sdn pod/sdn-97nhs pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:03.163 I ns/openshift-sdn pod/sdn-kq466 pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:04.063 I ns/openshift-sdn pod/sdn-7x2ns pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:04.790 I ns/openshift-multus pod/multus-5p7wr Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a58730174dabd295551c090ac657b0f7e5d3c93a732c7a57015d8a8d9040e55e"
Mar 04 20:39:04.954 I ns/openshift-multus pod/multus-5p7wr Created container
Mar 04 20:39:04.979 I ns/openshift-multus pod/multus-5p7wr Started container
Mar 04 20:39:05.174 I ns/openshift-multus pod/multus-5p7wr pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a892d084c92d90d10d2829e1dd7c0148f95893ff5f4749e9f50c313e2cdb040d"
Mar 04 20:39:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:39:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:39:07.852 W node/ip-10-0-136-28.us-west-1.compute.internal node is not ready
Mar 04 20:39:07.852 W node/ip-10-0-133-9.us-west-1.compute.internal node is not ready
Mar 04 20:39:07.852 W node/ip-10-0-156-96.us-west-1.compute.internal node is not ready
Mar 04 20:39:10.466 I ns/kube-system pod/etcd-member-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:39:12.411 I ns/openshift-multus pod/multus-pv6fw Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d"
Mar 04 20:39:12.446 I ns/openshift-sdn pod/ovs-cmzgh Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:12.455 I ns/openshift-sdn pod/sdn-controller-p66zk Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58"
Mar 04 20:39:12.462 I ns/openshift-sdn pod/sdn-7x2ns Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:12.633 I ns/openshift-multus pod/multus-pv6fw Created container
Mar 04 20:39:12.670 I ns/openshift-multus pod/multus-pv6fw Started container
Mar 04 20:39:12.723 I ns/openshift-sdn pod/sdn-controller-p66zk Created container
Mar 04 20:39:12.731 I ns/openshift-sdn pod/ovs-cmzgh Created container
Mar 04 20:39:12.751 I ns/openshift-sdn pod/sdn-7x2ns Created container
Mar 04 20:39:12.762 I ns/openshift-sdn pod/ovs-cmzgh Started container
Mar 04 20:39:12.774 I ns/openshift-sdn pod/sdn-controller-p66zk Started container
Mar 04 20:39:12.816 I ns/openshift-sdn pod/sdn-7x2ns Started container
Mar 04 20:39:13.014 I ns/openshift-multus pod/multus-pv6fw Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d" already present on machine
Mar 04 20:39:13.113 I ns/openshift-sdn configmap/openshift-network-controller ip-10-0-156-96 became leader
Mar 04 20:39:13.215 I ns/openshift-multus pod/multus-pv6fw Created container (2 times)
Mar 04 20:39:13.414 I ns/openshift-multus pod/multus-pv6fw Started container (2 times)
Mar 04 20:39:13.474 I ns/openshift-sdn pod/ovs-9mj4c Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:13.482 I ns/openshift-multus pod/multus-c765r Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d"
Mar 04 20:39:13.489 I ns/openshift-sdn pod/sdn-controller-7s46m Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58"
Mar 04 20:39:13.496 I ns/openshift-sdn pod/sdn-97nhs Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:13.708 I ns/openshift-sdn pod/sdn-controller-7s46m Created container
Mar 04 20:39:13.716 I ns/openshift-sdn pod/sdn-controller-7s46m Started container
Mar 04 20:39:13.729 I ns/openshift-sdn pod/ovs-9mj4c Created container
Mar 04 20:39:13.774 I ns/openshift-sdn pod/ovs-9mj4c Started container
Mar 04 20:39:13.817 I ns/openshift-sdn pod/sdn-97nhs Created container
Mar 04 20:39:13.836 I ns/openshift-multus pod/multus-c765r Created container
Mar 04 20:39:13.840 I ns/openshift-sdn pod/sdn-7x2ns Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867" already present on machine
Mar 04 20:39:13.849 W ns/openshift-multus pod/multus-pv6fw Back-off restarting failed container
Mar 04 20:39:13.865 W ns/openshift-multus pod/multus-pv6fw node/ip-10-0-156-96.us-west-1.compute.internal container=kube-multus container restarted
Mar 04 20:39:13.867 I ns/openshift-sdn pod/sdn-97nhs Started container
Mar 04 20:39:13.875 I ns/openshift-multus pod/multus-c765r Started container
Mar 04 20:39:14.032 I ns/openshift-sdn pod/sdn-7x2ns Created container (2 times)
Mar 04 20:39:14.085 I ns/openshift-multus pod/multus-c765r Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d" already present on machine
Mar 04 20:39:14.214 I ns/openshift-sdn pod/sdn-7x2ns Started container (2 times)
Mar 04 20:39:14.267 I node/ip-10-0-133-9.us-west-1.compute.internal Starting openshift-sdn.
Mar 04 20:39:14.275 I ns/openshift-multus pod/multus-c765r Created container (2 times)
Mar 04 20:39:14.308 I ns/openshift-sdn pod/ovs-8gj4v Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:14.315 I ns/openshift-multus pod/multus-5p7wr Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a892d084c92d90d10d2829e1dd7c0148f95893ff5f4749e9f50c313e2cdb040d"
Mar 04 20:39:14.321 I ns/openshift-sdn pod/sdn-kq466 Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:d9c5c60aa6fa136e7396c082c6e31e45472c4a55d6acd4b95ac38211121fe867"
Mar 04 20:39:14.337 I ns/openshift-sdn pod/sdn-controller-gf8rh Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58"
Mar 04 20:39:14.369 I node/ip-10-0-156-96.us-west-1.compute.internal Starting openshift-sdn.
Mar 04 20:39:14.478 I ns/openshift-multus pod/multus-c765r Started container (2 times)
Mar 04 20:39:14.577 I ns/openshift-multus pod/multus-5p7wr Created container
Mar 04 20:39:14.603 I ns/openshift-sdn pod/ovs-8gj4v Created container
Mar 04 20:39:14.619 I ns/openshift-multus pod/multus-5p7wr Started container
Mar 04 20:39:14.628 I ns/openshift-sdn pod/sdn-kq466 Created container
Mar 04 20:39:14.668 I ns/openshift-sdn pod/ovs-8gj4v Started container
Mar 04 20:39:14.678 I ns/openshift-sdn pod/sdn-controller-gf8rh Created container
Mar 04 20:39:14.683 I ns/openshift-sdn pod/sdn-kq466 Started container
Mar 04 20:39:14.711 I ns/openshift-sdn pod/sdn-controller-gf8rh Started container
Mar 04 20:39:14.859 W ns/openshift-multus pod/multus-pv6fw Back-off restarting failed container (2 times)
Mar 04 20:39:14.882 W ns/openshift-sdn pod/sdn-7x2ns node/ip-10-0-156-96.us-west-1.compute.internal container=sdn container restarted
Mar 04 20:39:15.040 I node/ip-10-0-136-28.us-west-1.compute.internal Starting openshift-sdn.
Mar 04 20:39:15.062 W ns/openshift-multus pod/multus-c765r node/ip-10-0-133-9.us-west-1.compute.internal container=kube-multus container restarted
Mar 04 20:39:15.323 I ns/openshift-multus pod/multus-5p7wr pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d"
Mar 04 20:39:19.485 W node/ip-10-0-136-28.us-west-1.compute.internal condition Ready changed
Mar 04 20:39:20.094 I ns/openshift-multus pod/multus-5p7wr Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d"
Mar 04 20:39:20.258 I ns/openshift-multus pod/multus-5p7wr Created container
Mar 04 20:39:20.281 I ns/openshift-multus pod/multus-5p7wr Started container
Mar 04 20:39:21.828 W node/ip-10-0-133-9.us-west-1.compute.internal condition Ready changed
Mar 04 20:39:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:39:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:39:22.852 W node/ip-10-0-156-96.us-west-1.compute.internal node is not ready
Mar 04 20:39:23.365 W node/ip-10-0-156-96.us-west-1.compute.internal condition Ready changed
Mar 04 20:39:27.430 I ns/openshift-multus pod/multus-pv6fw Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7403f9ce01dd3f48a1068f03c892b78f928e7ef5b58ca1972e76089940b4f6d" already present on machine (2 times)
Mar 04 20:39:27.596 I ns/openshift-multus pod/multus-pv6fw Created container (3 times)
Mar 04 20:39:27.621 I ns/openshift-multus pod/multus-pv6fw Started container (3 times)
Mar 04 20:39:27.917 W ns/openshift-multus pod/multus-pv6fw node/ip-10-0-156-96.us-west-1.compute.internal container=kube-multus container restarted
Mar 04 20:39:27.951 W clusteroperator/network changed Progressing to False
Mar 04 20:39:27.951 W clusteroperator/network changed Available to True
Mar 04 20:39:30.131 I ns/openshift-dns-operator deployment/dns-operator Scaled up replica set dns-operator-f6fc58c48 to 1
Mar 04 20:39:30.145 I ns/openshift-dns-operator pod/dns-operator-f6fc58c48-z87qh node/ created
Mar 04 20:39:30.160 I ns/openshift-dns-operator replicaset/dns-operator-f6fc58c48 Created pod: dns-operator-f6fc58c48-z87qh
Mar 04 20:39:30.172 I ns/openshift-dns-operator pod/dns-operator-f6fc58c48-z87qh Successfully assigned openshift-dns-operator/dns-operator-f6fc58c48-z87qh to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:39:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:39:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:39:37.976 I ns/openshift-dns-operator pod/dns-operator-f6fc58c48-z87qh pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a68ca73ae7f9153b31f96a83d78f2f342c24c85f9c859aaff9443b8e333c8c46"
Mar 04 20:39:44.853 E kube-apiserver Kube API started failing: Get https://api.sjenning.devcluster.openshift.com:6443/api/v1/namespaces/kube-system?timeout=3s: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
Mar 04 20:39:47.185 I ns/openshift-dns-operator pod/dns-operator-f6fc58c48-z87qh Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a68ca73ae7f9153b31f96a83d78f2f342c24c85f9c859aaff9443b8e333c8c46"
Mar 04 20:39:47.204 I ns/openshift-dns-operator pod/dns-operator-f6fc58c48-z87qh Created container
Mar 04 20:39:47.227 I ns/openshift-dns-operator pod/dns-operator-f6fc58c48-z87qh Started container
Mar 04 20:39:48.903 I kube-apiserver Kube API started responding to GET requests
Mar 04 20:39:50.089 I ns/openshift-dns pod/dns-default-2wvj7 node/ created
Mar 04 20:39:50.098 I ns/openshift-dns pod/dns-default-v4hv2 node/ created
Mar 04 20:39:50.099 I ns/openshift-dns daemonset/dns-default Created pod: dns-default-2wvj7
Mar 04 20:39:50.133 I ns/openshift-dns daemonset/dns-default Created pod: dns-default-v4hv2
Mar 04 20:39:50.133 I ns/openshift-dns pod/dns-default-2wvj7 Successfully assigned openshift-dns/dns-default-2wvj7 to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:39:50.133 I ns/openshift-dns pod/dns-default-2l56s node/ created
Mar 04 20:39:50.142 I ns/openshift-dns daemonset/dns-default Created pod: dns-default-2l56s
Mar 04 20:39:50.142 I ns/openshift-dns pod/dns-default-v4hv2 Successfully assigned openshift-dns/dns-default-v4hv2 to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:39:50.154 I ns/openshift-dns pod/dns-default-2l56s Successfully assigned openshift-dns/dns-default-2l56s to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:39:52.607 I clusteroperator/dns created
Mar 04 20:39:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:39:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:39:57.569 I ns/openshift-dns pod/dns-default-2wvj7 pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:27a8b22668ff30d562c9314ce39b1a45818ab537c8e1e2f04c8340a90ec45462"
Mar 04 20:39:58.137 I ns/openshift-dns pod/dns-default-v4hv2 pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:27a8b22668ff30d562c9314ce39b1a45818ab537c8e1e2f04c8340a90ec45462"
Mar 04 20:39:59.150 I ns/openshift-dns pod/dns-default-2l56s pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:27a8b22668ff30d562c9314ce39b1a45818ab537c8e1e2f04c8340a90ec45462"
Mar 04 20:40:02.640 I ns/openshift-dns pod/dns-default-2wvj7 Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:27a8b22668ff30d562c9314ce39b1a45818ab537c8e1e2f04c8340a90ec45462"
Mar 04 20:40:02.821 I ns/openshift-dns pod/dns-default-2wvj7 Created container
Mar 04 20:40:02.842 I ns/openshift-dns pod/dns-default-2wvj7 Started container
Mar 04 20:40:02.848 I ns/openshift-dns pod/dns-default-2wvj7 pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7cb9db9611c92843976c5ac064107bd94f1a92b8d9d9ca5039edb54eee400ce"
Mar 04 20:40:02.876 I ns/openshift-dns pod/dns-default-v4hv2 Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:27a8b22668ff30d562c9314ce39b1a45818ab537c8e1e2f04c8340a90ec45462"
Mar 04 20:40:03.044 I ns/openshift-dns pod/dns-default-v4hv2 Created container
Mar 04 20:40:03.064 I ns/openshift-dns pod/dns-default-v4hv2 Started container
Mar 04 20:40:03.069 I ns/openshift-dns pod/dns-default-v4hv2 pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7cb9db9611c92843976c5ac064107bd94f1a92b8d9d9ca5039edb54eee400ce"
Mar 04 20:40:04.095 I ns/openshift-dns pod/dns-default-2l56s Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:27a8b22668ff30d562c9314ce39b1a45818ab537c8e1e2f04c8340a90ec45462"
Mar 04 20:40:04.276 I ns/openshift-dns pod/dns-default-2l56s Created container
Mar 04 20:40:04.297 I ns/openshift-dns pod/dns-default-2l56s Started container
Mar 04 20:40:04.301 I ns/openshift-dns pod/dns-default-2l56s pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7cb9db9611c92843976c5ac064107bd94f1a92b8d9d9ca5039edb54eee400ce"
Mar 04 20:40:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:40:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:40:09.102 I ns/openshift-dns pod/dns-default-2wvj7 Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7cb9db9611c92843976c5ac064107bd94f1a92b8d9d9ca5039edb54eee400ce"
Mar 04 20:40:09.275 I ns/openshift-dns pod/dns-default-2wvj7 Created container
Mar 04 20:40:09.296 I ns/openshift-dns pod/dns-default-2wvj7 Started container
Mar 04 20:40:09.360 I ns/openshift-dns pod/dns-default-v4hv2 Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7cb9db9611c92843976c5ac064107bd94f1a92b8d9d9ca5039edb54eee400ce"
Mar 04 20:40:09.516 I ns/openshift-dns pod/dns-default-v4hv2 Created container
Mar 04 20:40:09.538 I ns/openshift-dns pod/dns-default-v4hv2 Started container
Mar 04 20:40:10.487 I ns/openshift-dns pod/dns-default-2l56s Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e7cb9db9611c92843976c5ac064107bd94f1a92b8d9d9ca5039edb54eee400ce"
Mar 04 20:40:10.653 I ns/openshift-dns pod/dns-default-2l56s Created container
Mar 04 20:40:10.676 I ns/openshift-dns pod/dns-default-2l56s Started container
Mar 04 20:40:11.171 W clusteroperator/dns changed Available to True
Mar 04 20:40:12.188 I ns/openshift-cluster-machine-approver deployment/machine-approver Scaled up replica set machine-approver-84b4fbc955 to 1
Mar 04 20:40:12.198 I ns/openshift-cluster-machine-approver pod/machine-approver-84b4fbc955-rhr9b node/ created
Mar 04 20:40:12.213 I ns/openshift-cluster-machine-approver replicaset/machine-approver-84b4fbc955 Created pod: machine-approver-84b4fbc955-rhr9b
Mar 04 20:40:12.228 I ns/openshift-cluster-machine-approver pod/machine-approver-84b4fbc955-rhr9b Successfully assigned openshift-cluster-machine-approver/machine-approver-84b4fbc955-rhr9b to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:40:12.728 I ns/openshift-cluster-machine-approver pod/machine-approver-84b4fbc955-rhr9b pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e2ff0f0306a71be3dbb044e4cf6b44891b6a6fea9aa658a5937ecbb5917d2046"
Mar 04 20:40:13.308 I ns/openshift-core-operators deployment/openshift-service-cert-signer-operator Scaled up replica set openshift-service-cert-signer-operator-6d87f7758f to 1
Mar 04 20:40:13.323 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Scaled up replica set openshift-service-ca-operator-96d894ddb to 1
Mar 04 20:40:13.337 I ns/openshift-service-ca-operator pod/openshift-service-ca-operator-96d894ddb-v7qgd node/ created
Mar 04 20:40:13.355 I ns/openshift-service-ca-operator replicaset/openshift-service-ca-operator-96d894ddb Created pod: openshift-service-ca-operator-96d894ddb-v7qgd
Mar 04 20:40:13.363 I ns/openshift-service-ca-operator pod/openshift-service-ca-operator-96d894ddb-v7qgd Successfully assigned openshift-service-ca-operator/openshift-service-ca-operator-96d894ddb-v7qgd to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:40:14.320 I ns/openshift-core-operators pod/openshift-service-cert-signer-operator-6d87f7758f-zvpvv node/ created
Mar 04 20:40:14.329 I ns/openshift-core-operators replicaset/openshift-service-cert-signer-operator-6d87f7758f Created pod: openshift-service-cert-signer-operator-6d87f7758f-zvpvv
Mar 04 20:40:14.336 I ns/openshift-core-operators pod/openshift-service-cert-signer-operator-6d87f7758f-zvpvv Successfully assigned openshift-core-operators/openshift-service-cert-signer-operator-6d87f7758f-zvpvv to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:40:15.337 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Scaled up replica set kube-apiserver-operator-7644449895 to 1
Mar 04 20:40:15.348 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q node/ created
Mar 04 20:40:15.377 I ns/openshift-kube-apiserver-operator replicaset/kube-apiserver-operator-7644449895 Created pod: kube-apiserver-operator-7644449895-bs45q
Mar 04 20:40:15.377 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Successfully assigned openshift-kube-apiserver-operator/kube-apiserver-operator-7644449895-bs45q to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:40:17.273 I ns/openshift-cluster-machine-approver pod/machine-approver-84b4fbc955-rhr9b Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:e2ff0f0306a71be3dbb044e4cf6b44891b6a6fea9aa658a5937ecbb5917d2046"
Mar 04 20:40:17.465 I ns/openshift-cluster-machine-approver pod/machine-approver-84b4fbc955-rhr9b Created container
Mar 04 20:40:17.484 I ns/openshift-cluster-machine-approver pod/machine-approver-84b4fbc955-rhr9b Started container
Mar 04 20:40:18.853 I ns/kube-system pod/etcd-member-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:40:21.227 I ns/openshift-service-ca-operator pod/openshift-service-ca-operator-96d894ddb-v7qgd pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8"
Mar 04 20:40:21.720 I ns/openshift-core-operators pod/openshift-service-cert-signer-operator-6d87f7758f-zvpvv pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:f7e13b9f3623118528998e507727c7b6bd5138e73fc61fee0934fbf34e9b3246"
Mar 04 20:40:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:40:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:40:23.554 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089"
Mar 04 20:40:28.274 I ns/openshift-service-ca-operator pod/openshift-service-ca-operator-96d894ddb-v7qgd Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8"
Mar 04 20:40:28.279 I ns/openshift-core-operators pod/openshift-service-cert-signer-operator-6d87f7758f-zvpvv Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:f7e13b9f3623118528998e507727c7b6bd5138e73fc61fee0934fbf34e9b3246"
Mar 04 20:40:28.502 I ns/openshift-service-ca-operator pod/openshift-service-ca-operator-96d894ddb-v7qgd Created container
Mar 04 20:40:28.524 I ns/openshift-core-operators pod/openshift-service-cert-signer-operator-6d87f7758f-zvpvv Created container
Mar 04 20:40:28.530 I ns/openshift-service-ca-operator pod/openshift-service-ca-operator-96d894ddb-v7qgd Started container
Mar 04 20:40:28.549 I ns/openshift-core-operators pod/openshift-service-cert-signer-operator-6d87f7758f-zvpvv Started container
Mar 04 20:40:28.600 I ns/openshift-core-operators configmap/openshift-service-cert-signer-operator-lock befd24d7-3ebd-11e9-bfa7-0a580a820005 became leader
Mar 04 20:40:29.218 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089"
Mar 04 20:40:29.392 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Created container
Mar 04 20:40:29.412 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Started container
Mar 04 20:40:29.713 I ns/openshift-service-ca-operator configmap/openshift-service-ca-operator-lock bfa965d0-3ebd-11e9-b419-0a580a820004 became leader
Mar 04 20:40:29.725 W ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Unable to determine current operator status for service-ca
Mar 04 20:40:29.731 I clusteroperator/service-ca created
Mar 04 20:40:30.924 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Namespace/openshift-service-cert-signer because it was missing
Mar 04 20:40:30.936 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ClusterRole.rbac.authorization.k8s.io/system:openshift:controller:service-serving-cert-signer because it was missing
Mar 04 20:40:30.949 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:controller:service-serving-cert-signer because it was missing
Mar 04 20:40:30.959 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Role.rbac.authorization.k8s.io/system:openshift:controller:service-serving-cert-signer -n openshift-service-cert-signer because it was missing
Mar 04 20:40:30.971 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created RoleBinding.rbac.authorization.k8s.io/system:openshift:controller:service-serving-cert-signer -n openshift-service-cert-signer because it was missing
Mar 04 20:40:31.060 I ns/openshift-kube-apiserver-operator configmap/openshift-cluster-kube-apiserver-operator-lock c07740c2-3ebd-11e9-ba26-0a580a800003 became leader
Mar 04 20:40:31.104 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Unable to determine current operator status for kube-apiserver
Mar 04 20:40:31.120 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no image found for operand pod
Mar 04 20:40:31.124 I clusteroperator/kube-apiserver created
Mar 04 20:40:31.125 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no observedConfig
Mar 04 20:40:31.129 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator new revision 1 triggered by "configmap \"kube-apiserver-pod\" not found"
Mar 04 20:40:31.136 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no observedConfig (2 times)
Mar 04 20:40:31.141 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no observedConfig (3 times)
Mar 04 20:40:31.148 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no image found for operand pod (2 times)
Mar 04 20:40:31.153 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no observedConfig (4 times)
Mar 04 20:40:31.159 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no observedConfig (5 times)
Mar 04 20:40:31.201 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Observed new master node ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:40:31.301 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Observed new master node ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:40:31.492 W clusteroperator/kube-apiserver changed Failing to False
Mar 04 20:40:31.492 W clusteroperator/kube-apiserver changed Progressing to False
Mar 04 20:40:31.492 W clusteroperator/kube-apiserver changed Available to False: Available: Available: 0 nodes are active; 
Mar 04 20:40:31.500 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Observed new master node ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:40:31.700 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Role.rbac.authorization.k8s.io/prometheus-k8s -n openshift-kube-apiserver because it was missing
Mar 04 20:40:31.723 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ServiceAccount/service-serving-cert-signer-sa -n openshift-service-cert-signer because it was missing
Mar 04 20:40:31.900 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created RoleBinding.rbac.authorization.k8s.io/prometheus-k8s -n openshift-kube-apiserver because it was missing
Mar 04 20:40:32.093 W clusteroperator/kube-apiserver changed Upgradeable to True
Mar 04 20:40:32.100 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource
Mar 04 20:40:32.300 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator "loadbalancer-serving-signer" in "openshift-kube-apiserver-operator" requires a new signing cert/key pair: missing notAfter
Mar 04 20:40:32.502 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator "service-network-serving-ca" in "openshift-kube-apiserver-operator" requires a new cert
Mar 04 20:40:32.524 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ConfigMap/service-serving-cert-signer-config -n openshift-service-cert-signer because it was missing
Mar 04 20:40:32.702 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator "localhost-serving-ca" in "openshift-kube-apiserver-operator" requires a new cert
Mar 04 20:40:32.902 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no observedConfig (6 times)
Mar 04 20:40:33.100 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Status for operator kube-apiserver changed: Failing changed from Unknown to False (""),Progressing changed from Unknown to False (""),Available changed from Unknown to False ("Available: 0 nodes are active; ")
Mar 04 20:40:33.304 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no image found for operand pod (3 times)
Mar 04 20:40:33.500 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator "kube-control-plane-signer-ca" in "openshift-kube-apiserver-operator" requires a new cert
Mar 04 20:40:33.702 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator "kube-control-plane-signer-ca" in "openshift-kube-apiserver-operator" requires a new cert (2 times)
Mar 04 20:40:33.902 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Observed new master node ip-10-0-133-9.us-west-1.compute.internal (2 times)
Mar 04 20:40:33.926 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Secret/service-serving-cert-signer-signing-key -n openshift-service-cert-signer because it was missing
Mar 04 20:40:33.940 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Deployment.apps/service-serving-cert-signer -n openshift-service-cert-signer because it was missing
Mar 04 20:40:33.954 I ns/openshift-service-cert-signer deployment/service-serving-cert-signer Scaled up replica set service-serving-cert-signer-7bfbfc584d to 1
Mar 04 20:40:33.956 I ns/openshift-service-cert-signer pod/service-serving-cert-signer-7bfbfc584d-8gk9w node/ created
Mar 04 20:40:33.963 I ns/openshift-service-cert-signer replicaset/service-serving-cert-signer-7bfbfc584d Created pod: service-serving-cert-signer-7bfbfc584d-8gk9w
Mar 04 20:40:33.976 I ns/openshift-service-cert-signer pod/service-serving-cert-signer-7bfbfc584d-8gk9w Successfully assigned openshift-service-cert-signer/service-serving-cert-signer-7bfbfc584d-8gk9w to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:40:34.102 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Observed new master node ip-10-0-156-96.us-west-1.compute.internal (2 times)
Mar 04 20:40:34.330 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ClusterRole.rbac.authorization.k8s.io/system:openshift:controller:apiservice-cabundle-injector because it was missing
Mar 04 20:40:34.335 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:controller:apiservice-cabundle-injector because it was missing
Mar 04 20:40:34.342 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Role.rbac.authorization.k8s.io/system:openshift:controller:apiservice-cabundle-injector -n openshift-service-cert-signer because it was missing
Mar 04 20:40:34.350 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created RoleBinding.rbac.authorization.k8s.io/system:openshift:controller:apiservice-cabundle-injector -n openshift-service-cert-signer because it was missing
Mar 04 20:40:34.924 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ServiceAccount/apiservice-cabundle-injector-sa -n openshift-service-cert-signer because it was missing
Mar 04 20:40:35.107 E clusteroperator/kube-apiserver changed Failing to True: RevisionControllerFailing: RevisionControllerFailing: configmaps "kube-apiserver-pod" not found
Mar 04 20:40:35.725 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ConfigMap/apiservice-cabundle-injector-config -n openshift-service-cert-signer because it was missing
Mar 04 20:40:36.926 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ConfigMap/signing-cabundle -n openshift-service-cert-signer because it was missing
Mar 04 20:40:36.936 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Deployment.apps/apiservice-cabundle-injector -n openshift-service-cert-signer because it was missing
Mar 04 20:40:36.944 I ns/openshift-service-cert-signer pod/apiservice-cabundle-injector-5f4dd8f5f5-n9245 node/ created
Mar 04 20:40:36.946 I ns/openshift-service-cert-signer deployment/apiservice-cabundle-injector Scaled up replica set apiservice-cabundle-injector-5f4dd8f5f5 to 1
Mar 04 20:40:36.950 I ns/openshift-service-cert-signer replicaset/apiservice-cabundle-injector-5f4dd8f5f5 Created pod: apiservice-cabundle-injector-5f4dd8f5f5-n9245
Mar 04 20:40:36.966 I ns/openshift-service-cert-signer pod/apiservice-cabundle-injector-5f4dd8f5f5-n9245 Successfully assigned openshift-service-cert-signer/apiservice-cabundle-injector-5f4dd8f5f5-n9245 to ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:40:37.187 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ConfigMap/service-ca -n openshift-config-managed because it was missing
Mar 04 20:40:37.329 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ClusterRole.rbac.authorization.k8s.io/system:openshift:controller:configmap-cabundle-injector because it was missing
Mar 04 20:40:37.336 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:controller:configmap-cabundle-injector because it was missing
Mar 04 20:40:37.346 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created Role.rbac.authorization.k8s.io/system:openshift:controller:configmap-cabundle-injector -n openshift-service-cert-signer because it was missing
Mar 04 20:40:37.351 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created RoleBinding.rbac.authorization.k8s.io/system:openshift:controller:configmap-cabundle-injector -n openshift-service-cert-signer because it was missing
Mar 04 20:40:37.724 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ServiceAccount/configmap-cabundle-injector-sa -n openshift-service-cert-signer because it was missing
Mar 04 20:40:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:40:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:40:38.125 I ns/openshift-service-ca-operator deployment/openshift-service-ca-operator Created ConfigMap/configmap-cabundle-injector-config -n openshift-service-cert-signer because it was missing
Mar 04 20:40:38.342 I ns/openshift-service-cert-signer pod/configmap-cabundle-injector-84cbfd64f5-wk6xq node/ created
Mar 04 20:40:38.343 I ns/openshift-service-cert-signer deployment/configmap-cabundle-injector Scaled up replica set configmap-cabundle-injector-84cbfd64f5 to 1
Mar 04 20:40:38.352 I ns/openshift-service-cert-signer replicaset/configmap-cabundle-injector-84cbfd64f5 Created pod: configmap-cabundle-injector-84cbfd64f5-wk6xq
Mar 04 20:40:38.360 I ns/openshift-service-cert-signer pod/configmap-cabundle-injector-84cbfd64f5-wk6xq Successfully assigned openshift-service-cert-signer/configmap-cabundle-injector-84cbfd64f5-wk6xq to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:40:41.577 I ns/openshift-service-cert-signer pod/service-serving-cert-signer-7bfbfc584d-8gk9w pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8"
Mar 04 20:40:44.229 I ns/openshift-service-cert-signer pod/apiservice-cabundle-injector-5f4dd8f5f5-n9245 Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8" already present on machine
Mar 04 20:40:44.413 I ns/openshift-service-cert-signer pod/apiservice-cabundle-injector-5f4dd8f5f5-n9245 Created container
Mar 04 20:40:44.435 I ns/openshift-service-cert-signer pod/apiservice-cabundle-injector-5f4dd8f5f5-n9245 Started container
Mar 04 20:40:45.643 I ns/openshift-service-cert-signer pod/configmap-cabundle-injector-84cbfd64f5-wk6xq pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8"
Mar 04 20:40:46.176 I ns/openshift-service-cert-signer configmap/openshift-service-serving-cert-signer-apiservice-injector-lock c9796625-3ebd-11e9-ae5a-0a580a820006 became leader
Mar 04 20:40:46.951 I ns/openshift-service-cert-signer pod/service-serving-cert-signer-7bfbfc584d-8gk9w Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8"
Mar 04 20:40:47.193 I ns/openshift-service-cert-signer pod/service-serving-cert-signer-7bfbfc584d-8gk9w Created container
Mar 04 20:40:47.198 I ns/openshift-service-cert-signer pod/service-serving-cert-signer-7bfbfc584d-8gk9w Started container
Mar 04 20:40:48.425 I ns/openshift-service-cert-signer configmap/openshift-service-serving-cert-signer-serving-ca-lock cad05840-3ebd-11e9-b14d-0a580a810003 became leader
Mar 04 20:40:50.746 I ns/openshift-service-cert-signer pod/configmap-cabundle-injector-84cbfd64f5-wk6xq Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:18eaa6322259d1bbe65d96068a9a91c3d14bf9626bc7318dc8c7df512dd907b8"
Mar 04 20:40:50.934 I ns/openshift-service-cert-signer pod/configmap-cabundle-injector-84cbfd64f5-wk6xq Created container
Mar 04 20:40:50.953 I ns/openshift-service-cert-signer pod/configmap-cabundle-injector-84cbfd64f5-wk6xq Started container
Mar 04 20:40:51.880 I ns/openshift-service-cert-signer configmap/openshift-service-serving-cert-signer-cabundle-injector-lock ccdfc128-3ebd-11e9-9e4e-0a580a800004 became leader
Mar 04 20:40:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:40:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:41:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:41:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:41:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:41:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:41:36.269 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089" already present on machine
Mar 04 20:41:36.274 W ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q node/ip-10-0-133-9.us-west-1.compute.internal container=operator container stopped being ready
Mar 04 20:41:36.453 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Created container (2 times)
Mar 04 20:41:36.476 I ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q Started container (2 times)
Mar 04 20:41:37.280 W ns/openshift-kube-apiserver-operator pod/kube-apiserver-operator-7644449895-bs45q node/ip-10-0-133-9.us-west-1.compute.internal container=operator container restarted
Mar 04 20:41:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:41:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:41:52.477 I ns/openshift-kube-apiserver-operator configmap/openshift-cluster-kube-apiserver-operator-lock e7ce0c6f-3ebd-11e9-923d-0a580a800003 became leader
Mar 04 20:41:52.502 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Unable to determine current operator status for kube-apiserver
Mar 04 20:41:52.505 W clusteroperator/kube-apiserver deleted
Mar 04 20:41:52.515 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator new revision 1 triggered by "configmap \"kube-apiserver-pod-0\" not found"
Mar 04 20:41:52.517 I clusteroperator/kube-apiserver created
Mar 04 20:41:52.605 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource
Mar 04 20:41:52.617 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (2 times)
Mar 04 20:41:52.635 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (3 times)
Mar 04 20:41:52.698 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (4 times)
Mar 04 20:41:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:41:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:41:53.098 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (5 times)
Mar 04 20:41:53.499 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (6 times)
Mar 04 20:41:53.904 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (7 times)
Mar 04 20:41:54.298 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (8 times)
Mar 04 20:41:54.947 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (9 times)
Mar 04 20:41:55.494 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no image found for operand pod
Mar 04 20:41:56.235 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (10 times)
Mar 04 20:41:56.301 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Secret/kubelet-client-1 -n openshift-kube-apiserver because it was missing
Mar 04 20:41:56.896 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Secret/serving-cert-1 -n openshift-kube-apiserver because it was missing
Mar 04 20:41:58.803 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (11 times)
Mar 04 20:42:03.504 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Revision 0 created because configmap "kube-apiserver-pod-0" not found
Mar 04 20:42:03.512 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (12 times)
Mar 04 20:42:03.516 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Status for operator kube-apiserver changed: Failing message changed from "StaticPodsFailing: pods \"kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal\" not found\nStaticPodsFailing: pods \"kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal\" not found\nStaticPodsFailing: pods \"kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal\" not found\nRevisionControllerFailing: configmaps \"kube-apiserver-pod\" not found" to "StaticPodsFailing: pods \"kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal\" not found\nStaticPodsFailing: pods \"kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal\" not found\nStaticPodsFailing: pods \"kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal\" not found"
Mar 04 20:42:03.935 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (13 times)
Mar 04 20:42:04.101 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Updating node "ip-10-0-133-9.us-west-1.compute.internal" from revision 0 to 1
Mar 04 20:42:04.107 W clusteroperator/kube-apiserver changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 0
Mar 04 20:42:04.112 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (14 times)
Mar 04 20:42:04.116 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Status for operator kube-apiserver changed: Progressing changed from False to True ("Progressing: 3 nodes are at revision 0")
Mar 04 20:42:06.096 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:42:06.100 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal -n openshift-kube-apiserver because it was missing
Mar 04 20:42:06.297 W ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator no image found for operand pod (2 times)
Mar 04 20:42:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:42:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:42:14.196 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089" already present on machine
Mar 04 20:42:14.360 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:42:14.379 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:42:17.866 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:42:18.029 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:42:18.352 W ns/openshift-kube-apiserver pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:42:18.373 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine
Mar 04 20:42:18.591 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:42:18.612 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:42:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:42:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:42:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:42:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:42:40.106 W clusteroperator/kube-apiserver changed Available to True: AsExpected: Available: 1 nodes are active; 1 nodes are at revision 1; 2 nodes are at revision 0
Mar 04 20:42:46.299 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:42:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:42:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:42:54.335 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089"
Mar 04 20:42:59.812 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089"
Mar 04 20:42:59.979 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Created container
Mar 04 20:43:00.000 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Started container
Mar 04 20:43:03.487 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:43:03.655 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:43:03.967 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine
Mar 04 20:43:04.147 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Created container
Mar 04 20:43:04.164 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Started container
Mar 04 20:43:04.199 W ns/openshift-kube-apiserver pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:43:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:43:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:43:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:43:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:43:26.500 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:43:33.900 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089"
Mar 04 20:43:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:43:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:43:39.368 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089"
Mar 04 20:43:39.546 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Created container
Mar 04 20:43:39.567 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Started container
Mar 04 20:43:41.642 E clusterversion/version changed Failing to True: ClusterOperatorFailing: Cluster operator kube-apiserver is reporting a failure: StaticPodsFailing: pods "kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal" not found
Mar 04 20:43:43.055 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:43:43.222 I ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:43:43.545 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine
Mar 04 20:43:43.724 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal Created container
Mar 04 20:43:43.748 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal Started container
Mar 04 20:43:43.780 W ns/openshift-kube-apiserver pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:43:44.105 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-1" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:43:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:43:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:43:56.306 W clusteroperator/kube-apiserver changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 1
Mar 04 20:44:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:44:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:44:09.218 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Scaled up replica set openshift-kube-scheduler-operator-65dd9877d5 to 1
Mar 04 20:44:09.226 I ns/openshift-kube-scheduler-operator pod/openshift-kube-scheduler-operator-65dd9877d5-sfvws node/ created
Mar 04 20:44:09.242 I ns/openshift-kube-scheduler-operator replicaset/openshift-kube-scheduler-operator-65dd9877d5 Created pod: openshift-kube-scheduler-operator-65dd9877d5-sfvws
Mar 04 20:44:09.307 I ns/openshift-kube-scheduler-operator pod/openshift-kube-scheduler-operator-65dd9877d5-sfvws Successfully assigned openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-65dd9877d5-sfvws to ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:44:11.642 W clusterversion/version changed Failing to False
Mar 04 20:44:16.603 I ns/openshift-kube-scheduler-operator pod/openshift-kube-scheduler-operator-65dd9877d5-sfvws pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d"
Mar 04 20:44:22.114 I ns/openshift-kube-scheduler-operator pod/openshift-kube-scheduler-operator-65dd9877d5-sfvws Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d"
Mar 04 20:44:22.299 I ns/openshift-kube-scheduler-operator pod/openshift-kube-scheduler-operator-65dd9877d5-sfvws Created container
Mar 04 20:44:22.320 I ns/openshift-kube-scheduler-operator pod/openshift-kube-scheduler-operator-65dd9877d5-sfvws Started container
Mar 04 20:44:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:44:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:44:23.001 I ns/openshift-kube-scheduler-operator configmap/openshift-cluster-kube-scheduler-operator-lock 4ab67c70-3ebe-11e9-b68e-0a580a800006 became leader
Mar 04 20:44:23.024 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Unable to determine current operator status for kube-scheduler
Mar 04 20:44:23.034 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator no image found for operand pod
Mar 04 20:44:23.036 I clusteroperator/kube-scheduler created
Mar 04 20:44:23.039 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator new revision 1 triggered by "configmap \"kube-scheduler-pod\" not found"
Mar 04 20:44:23.051 W clusteroperator/kube-scheduler changed Failing to False
Mar 04 20:44:23.051 W clusteroperator/kube-scheduler changed Progressing to False
Mar 04 20:44:23.051 W clusteroperator/kube-scheduler changed Available to False: Available: Available: 0 nodes are active; 
Mar 04 20:44:23.059 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator no image found for operand pod (2 times)
Mar 04 20:44:23.064 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Status for operator kube-scheduler changed: Failing changed from Unknown to False (""),Progressing changed from Unknown to False (""),Available changed from Unknown to False ("Available: 0 nodes are active; ")
Mar 04 20:44:23.071 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator no image found for operand pod (3 times)
Mar 04 20:44:23.128 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator no image found for operand pod (4 times)
Mar 04 20:44:23.129 W clusteroperator/kube-scheduler changed Upgradeable to True
Mar 04 20:44:23.134 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Status for operator kube-scheduler changed: Upgradeable changed from Unknown to True ("")
Mar 04 20:44:23.140 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Created Role.rbac.authorization.k8s.io/prometheus-k8s -n openshift-kube-scheduler because it was missing
Mar 04 20:44:23.148 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator no image found for operand pod (5 times)
Mar 04 20:44:23.219 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Created RoleBinding.rbac.authorization.k8s.io/prometheus-k8s -n openshift-kube-scheduler because it was missing
Mar 04 20:44:23.419 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource
Mar 04 20:44:23.621 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator no image found for operand pod (6 times)
Mar 04 20:44:23.821 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (2 times)
Mar 04 20:44:24.021 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (3 times)
Mar 04 20:44:24.223 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Status for operator kube-scheduler changed: Failing message changed from "" to "MonitoringResourceControllerFailing: the server could not find the requested resource"
Mar 04 20:44:24.421 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (4 times)
Mar 04 20:44:24.621 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (5 times)
Mar 04 20:44:24.821 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Observed new master node ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:44:25.020 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Observed new master node ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:44:25.219 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Observed new master node ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:44:25.419 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Status for operator kube-scheduler changed: Progressing message changed from "" to "Progressing: 3 nodes are at revision 0",Available message changed from "Available: 0 nodes are active; " to "Available: 0 nodes are active; 3 nodes are at revision 0"
Mar 04 20:44:25.620 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (6 times)
Mar 04 20:44:25.821 W ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (7 times)
Mar 04 20:44:26.022 I ns/openshift-kube-scheduler-operator deployment/openshift-kube-scheduler-operator Created ConfigMap/revision-status-1 -n openshift-kube-scheduler because it was missing
Mar 04 20:44:26.029 E clusteroperator/kube-scheduler changed Failing to True: RevisionControllerFailing: RevisionControllerFailing: configmaps "kube-scheduler-pod" not found
Mar 04 20:44:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:44:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:44:40.627 W clusteroperator/kube-scheduler changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 0
Mar 04 20:44:42.418 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:44:50.364 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d" already present on machine
Mar 04 20:44:50.529 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:44:50.550 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:44:50.641 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:44:50.677 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:44:51.172 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3"
Mar 04 20:44:51.583 W ns/openshift-kube-scheduler pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:44:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:44:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:44:59.179 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3"
Mar 04 20:44:59.348 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:44:59.369 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:45:00.431 W clusteroperator/kube-scheduler changed Available to True: AsExpected: Available: 1 nodes are active; 1 nodes are at revision 1; 2 nodes are at revision 0
Mar 04 20:45:06.622 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:45:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:45:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:45:13.907 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d"
Mar 04 20:45:19.424 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d"
Mar 04 20:45:19.599 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Created container
Mar 04 20:45:19.622 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Started container
Mar 04 20:45:19.718 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-156-96.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:45:19.757 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:45:20.255 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3"
Mar 04 20:45:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:45:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:45:29.004 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3"
Mar 04 20:45:30.106 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal Created container
Mar 04 20:45:30.150 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal Started container
Mar 04 20:45:34.817 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:45:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:45:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:45:42.373 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d"
Mar 04 20:45:47.576 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:a6c8ff98afa8f4cceb876b7905c7bb3495ab6663bc834e70f526e1687e22244d"
Mar 04 20:45:47.751 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Created container
Mar 04 20:45:47.775 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Started container
Mar 04 20:45:47.863 I ns/openshift-kube-scheduler pod/installer-1-ip-10-0-136-28.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:45:47.911 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:45:47.935 W clusteroperator/kube-scheduler changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:45:48.414 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3"
Mar 04 20:45:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:45:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:45:56.274 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3"
Mar 04 20:45:56.462 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal Created container
Mar 04 20:45:56.486 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal Started container
Mar 04 20:45:57.428 W clusteroperator/kube-scheduler changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 1
Mar 04 20:45:59.459 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Scaled up replica set kube-controller-manager-operator-5b6978b467 to 1
Mar 04 20:45:59.473 I ns/openshift-kube-controller-manager-operator pod/kube-controller-manager-operator-5b6978b467-z6n6z node/ created
Mar 04 20:45:59.491 I ns/openshift-kube-controller-manager-operator replicaset/kube-controller-manager-operator-5b6978b467 Created pod: kube-controller-manager-operator-5b6978b467-z6n6z
Mar 04 20:45:59.508 I ns/openshift-kube-controller-manager-operator pod/kube-controller-manager-operator-5b6978b467-z6n6z Successfully assigned openshift-kube-controller-manager-operator/kube-controller-manager-operator-5b6978b467-z6n6z to ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:46:07.341 I ns/openshift-kube-controller-manager-operator pod/kube-controller-manager-operator-5b6978b467-z6n6z pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:9e26ef58f89454604667888ffd6906140c02e9b968d9c3a91905e86814e520d9"
Mar 04 20:46:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:46:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:46:12.403 I ns/openshift-kube-controller-manager-operator pod/kube-controller-manager-operator-5b6978b467-z6n6z Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:9e26ef58f89454604667888ffd6906140c02e9b968d9c3a91905e86814e520d9"
Mar 04 20:46:12.582 I ns/openshift-kube-controller-manager-operator pod/kube-controller-manager-operator-5b6978b467-z6n6z Created container
Mar 04 20:46:12.606 I ns/openshift-kube-controller-manager-operator pod/kube-controller-manager-operator-5b6978b467-z6n6z Started container
Mar 04 20:46:13.229 I ns/openshift-kube-controller-manager-operator configmap/openshift-cluster-kube-controller-manager-operator-lock 8c67ff72-3ebe-11e9-a28d-0a580a810006 became leader
Mar 04 20:46:13.292 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Unable to determine current operator status for kube-controller-manager
Mar 04 20:46:13.317 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator no image found for operand pod
Mar 04 20:46:13.323 I clusteroperator/kube-controller-manager created
Mar 04 20:46:13.326 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator new revision 1 triggered by "configmap \"kube-controller-manager-pod\" not found"
Mar 04 20:46:13.335 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator no image found for operand pod (2 times)
Mar 04 20:46:13.342 W clusteroperator/kube-controller-manager changed Failing to False
Mar 04 20:46:13.342 W clusteroperator/kube-controller-manager changed Progressing to False
Mar 04 20:46:13.342 W clusteroperator/kube-controller-manager changed Available to False: Available: Available: 0 nodes are active; 
Mar 04 20:46:13.348 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Status for operator kube-controller-manager changed: Failing changed from Unknown to False (""),Progressing changed from Unknown to False (""),Available changed from Unknown to False ("Available: 0 nodes are active; ")
Mar 04 20:46:13.378 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Writing updated observed config: {\n\nA: }\n\nB: "extendedArguments":{"cloud-provider":["aws"],"cluster-cidr":["10.128.0.0/14"],"service-cluster-ip-range":["172.30.0.0/16"]}}\n\n
Mar 04 20:46:13.384 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Observed new master node ip-10-0-133-9.us-west-1.compute.internal
Mar 04 20:46:13.390 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Observed new master node ip-10-0-136-28.us-west-1.compute.internal
Mar 04 20:46:13.395 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Observed new master node ip-10-0-156-96.us-west-1.compute.internal
Mar 04 20:46:13.400 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator "csr-controller-signer-ca" in "openshift-kube-controller-manager-operator" requires a new cert
Mar 04 20:46:13.450 W clusteroperator/kube-controller-manager changed Upgradeable to True
Mar 04 20:46:13.483 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator no image found for operand pod (3 times)
Mar 04 20:46:13.678 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Observed new master node ip-10-0-133-9.us-west-1.compute.internal (2 times)
Mar 04 20:46:13.882 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Observed new master node ip-10-0-136-28.us-west-1.compute.internal (2 times)
Mar 04 20:46:14.083 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Observed new master node ip-10-0-156-96.us-west-1.compute.internal (2 times)
Mar 04 20:46:14.276 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Created Role.rbac.authorization.k8s.io/prometheus-k8s -n openshift-kube-controller-manager because it was missing
Mar 04 20:46:14.475 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Created RoleBinding.rbac.authorization.k8s.io/prometheus-k8s -n openshift-kube-controller-manager because it was missing
Mar 04 20:46:14.676 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource
Mar 04 20:46:14.875 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Status for operator kube-controller-manager changed: Upgradeable changed from Unknown to True ("")
Mar 04 20:46:15.076 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Status for operator kube-controller-manager changed: Progressing message changed from "" to "Progressing: 3 nodes are at revision 0",Available message changed from "Available: 0 nodes are active; " to "Available: 0 nodes are active; 3 nodes are at revision 0"
Mar 04 20:46:15.275 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Created ConfigMap/revision-status-1 -n openshift-kube-controller-manager because it was missing
Mar 04 20:46:15.476 I ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Status for operator kube-controller-manager changed: Failing message changed from "" to "MonitoringResourceControllerFailing: the server could not find the requested resource"
Mar 04 20:46:15.679 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (2 times)
Mar 04 20:46:15.879 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (3 times)
Mar 04 20:46:15.890 E clusteroperator/kube-controller-manager changed Failing to True: RevisionControllerFailing: RevisionControllerFailing: configmaps "kube-controller-manager-pod" not found
Mar 04 20:46:16.078 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (4 times)
Mar 04 20:46:16.281 W ns/openshift-kube-controller-manager-operator deployment/kube-controller-manager-operator Failed to create ServiceMonitor.monitoring.coreos.com/v1: the server could not find the requested resource (5 times)
Mar 04 20:46:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:46:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:46:22.910 E clusteroperator/kube-apiserver changed Failing to True: RevisionControllerFailing: RevisionControllerFailing: configmaps "kubelet-serving-ca" not found
Mar 04 20:46:37.690 W clusteroperator/kube-controller-manager changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 0
Mar 04 20:46:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:46:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:46:40.080 I ns/openshift-kube-controller-manager pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:46:47.705 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:46:47.831 I ns/openshift-kube-controller-manager pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal pulling image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:9e26ef58f89454604667888ffd6906140c02e9b968d9c3a91905e86814e520d9"
Mar 04 20:46:52.106 W clusteroperator/kube-apiserver changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 1
Mar 04 20:46:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:46:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:46:53.654 I ns/openshift-kube-controller-manager pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Successfully pulled image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:9e26ef58f89454604667888ffd6906140c02e9b968d9c3a91905e86814e520d9"
Mar 04 20:46:53.819 I ns/openshift-kube-controller-manager pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:46:53.840 I ns/openshift-kube-controller-manager pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:46:53.991 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:46:54.091 I ns/openshift-kube-controller-manager pod/installer-1-ip-10-0-133-9.us-west-1.compute.internal Successfully installed revision 1
Mar 04 20:46:54.100 I ns/openshift-kube-apiserver pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:46:54.103 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal -n openshift-kube-apiserver because it was missing
Mar 04 20:46:54.293 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:46:54.511 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3" already present on machine
Mar 04 20:46:54.728 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:46:54.758 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:46:59.801 I ns/openshift-kube-controller-manager pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:47:00.600 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:47:01.721 I ns/openshift-kube-apiserver pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089" already present on machine
Mar 04 20:47:01.901 I ns/openshift-kube-apiserver pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:47:01.927 I ns/openshift-kube-apiserver pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:47:02.343 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089" already present on machine
Mar 04 20:47:02.499 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:47:02.522 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:47:05.996 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:47:06.059 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:47:06.068 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:47:06.079 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:47:06.179 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Successfully installed revision 2
Mar 04 20:47:06.587 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine
Mar 04 20:47:06.782 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:47:06.814 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:47:06.862 W ns/openshift-kube-apiserver pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:47:07.142 I ns/openshift-kube-controller-manager pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:9e26ef58f89454604667888ffd6906140c02e9b968d9c3a91905e86814e520d9" already present on machine
Mar 04 20:47:07.319 I ns/openshift-kube-controller-manager pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:47:07.339 I ns/openshift-kube-controller-manager pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:47:07.832 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine (2 times)
Mar 04 20:47:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:47:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:47:08.083 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Created container (2 times)
Mar 04 20:47:08.110 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Started container (2 times)
Mar 04 20:47:08.415 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:9e26ef58f89454604667888ffd6906140c02e9b968d9c3a91905e86814e520d9" already present on machine
Mar 04 20:47:08.585 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:47:08.607 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:47:08.717 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:47:08.730 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:47:08.736 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:47:08.753 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:47:08.847 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container
Mar 04 20:47:08.861 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-2 container restarted
Mar 04 20:47:08.863 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal Successfully installed revision 2
Mar 04 20:47:09.142 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Killing container with id cri-o://kube-apiserver-1:Need to kill Pod
Mar 04 20:47:09.261 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3" already present on machine
Mar 04 20:47:09.602 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Created container
Mar 04 20:47:09.608 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Started container
Mar 04 20:47:09.853 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (2 times)
Mar 04 20:47:09.901 W ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:47:10.868 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3" already present on machine (2 times)
Mar 04 20:47:11.043 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Created container (2 times)
Mar 04 20:47:11.067 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Started container (2 times)
Mar 04 20:47:11.509 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-2" is terminated: "Error" - "egator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 20:47:08.251298       1 server.go:692] external host was not specified, using 10.0.133.9\nI0304 20:47:08.251767       1 server.go:152] Version: v1.12.4+761b685\nF0304 20:47:08.251920       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 20:47:11.883 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-2 container restarted
Mar 04 20:47:12.876 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container
Mar 04 20:47:13.883 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (2 times)
Mar 04 20:47:15.338 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (3 times)
Mar 04 20:47:18.981 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (3 times)
Mar 04 20:47:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:47:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:47:27.463 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine (3 times)
Mar 04 20:47:27.644 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Created container (3 times)
Mar 04 20:47:27.664 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal Started container (3 times)
Mar 04 20:47:27.914 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-2 container restarted
Mar 04 20:47:30.109 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-2" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:47:30.470 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3" already present on machine (3 times)
Mar 04 20:47:30.675 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Created container (3 times)
Mar 04 20:47:30.702 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Started container (3 times)
Mar 04 20:47:30.927 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-2 container restarted
Mar 04 20:47:32.933 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (4 times)
Mar 04 20:47:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:47:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:47:38.872 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Killing container with id cri-o://kube-controller-manager-1:Need to kill Pod
Mar 04 20:47:38.982 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (5 times)
Mar 04 20:47:51.465 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Back-off restarting failed container (6 times)
Mar 04 20:47:52.499 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:47:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:47:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:47:59.994 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:112f6e6012b1f863f53bb8726594238bd27a96618ebda83f295f70b04af13089" already present on machine
Mar 04 20:48:00.167 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal Created container
Mar 04 20:48:00.189 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal Started container
Mar 04 20:48:03.658 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:48:03.681 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:48:03.686 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:48:03.700 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:48:03.839 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal Successfully installed revision 2
Mar 04 20:48:04.185 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine
Mar 04 20:48:04.352 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Created container
Mar 04 20:48:04.374 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Started container
Mar 04 20:48:04.464 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:50ba75a349990993786f4bab77f51d7f4393130ada572549b796a431559a2ab3" already present on machine (4 times)
Mar 04 20:48:04.506 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: pods "kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal" not found
Mar 04 20:48:04.652 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Created container (4 times)
Mar 04 20:48:04.671 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal Started container (4 times)
Mar 04 20:48:04.684 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine (2 times)
Mar 04 20:48:04.701 W ns/openshift-kube-apiserver pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:48:04.880 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Created container (2 times)
Mar 04 20:48:04.901 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Started container (2 times)
Mar 04 20:48:05.081 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-2 container restarted
Mar 04 20:48:05.689 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Back-off restarting failed container
Mar 04 20:48:05.696 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-2 container restarted
Mar 04 20:48:06.702 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Back-off restarting failed container (2 times)
Mar 04 20:48:06.804 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Killing container with id cri-o://kube-apiserver-1:Need to kill Pod
Mar 04 20:48:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:48:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:48:09.794 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Back-off restarting failed container (3 times)
Mar 04 20:48:19.212 W clusteroperator/kube-controller-manager changed Available to True: AsExpected: Available: 1 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 0
Mar 04 20:48:22.427 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Container image "registry.svc.ci.openshift.org/openshift/origin-v4.0-2019-03-04-190542@sha256:41fbbb1ff204e79e491363ede13255cf1da29b5a28074f74bbf852d87a932e58" already present on machine (3 times)
Mar 04 20:48:22.602 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Created container (3 times)
Mar 04 20:48:22.625 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal Started container (3 times)
Mar 04 20:48:22.738 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-2 container restarted
Mar 04 20:48:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:48:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:48:24.112 I ns/kube-system / Required control plane pods have been created
Mar 04 20:48:24.910 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-2" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:48:24.977 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:48:30.783 I ns/openshift-kube-scheduler pod/revision-pruner-1-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:48:32.780 I ns/openshift-kube-scheduler pod/revision-pruner-1-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:48:34.780 I ns/openshift-kube-scheduler pod/revision-pruner-1-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:48:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:48:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:48:38.732 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:48:48.295 I ns/openshift-kube-apiserver pod/installer-2-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:48:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:48:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:48:55.578 I ns/openshift-kube-controller-manager pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:48:59.980 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:48:59.996 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:49:00.004 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:49:00.012 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:49:00.305 W ns/openshift-kube-apiserver pod/installer-2-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:49:01.521 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-2" is terminated: "Error" - "gator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 20:49:00.955737       1 server.go:692] external host was not specified, using 10.0.136.28\nI0304 20:49:00.956143       1 server.go:152] Version: v1.12.4+761b685\nF0304 20:49:00.956599       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 20:49:02.323 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-2 container restarted
Mar 04 20:49:04.066 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:49:04.096 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:49:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:49:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:49:10.701 I ns/openshift-kube-apiserver pod/installer-3-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:49:19.449 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-2 container restarted
Mar 04 20:49:22.288 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:49:22.354 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:49:22.358 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:49:22.369 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:49:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:49:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:49:23.466 W ns/openshift-kube-apiserver pod/installer-3-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:49:23.759 W clusteroperator/kube-controller-manager changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 2
Mar 04 20:49:24.475 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-3 container restarted
Mar 04 20:49:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:49:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:49:40.521 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-3 container restarted
Mar 04 20:49:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:49:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:49:58.310 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:50:07.300 I ns/openshift-kube-apiserver pod/revision-pruner-3-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:50:07.494 I ns/openshift-kube-apiserver pod/installer-3-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:50:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:50:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:50:18.620 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:50:18.634 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:50:18.642 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:50:18.656 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:50:19.354 W ns/openshift-kube-apiserver pod/installer-3-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:50:21.379 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-3 container restarted
Mar 04 20:50:21.985 E kube-apiserver failed contacting the API: Get https://api.sjenning.devcluster.openshift.com:6443/api/v1/pods?resourceVersion=5760&timeoutSeconds=519&watch=true: dial tcp 13.52.84.161:6443: connect: connection refused
Mar 04 20:50:21.991 E kube-apiserver failed contacting the API: Get https://api.sjenning.devcluster.openshift.com:6443/apis/config.openshift.io/v1/clusteroperators?resourceVersion=5587&timeoutSeconds=411&watch=true: dial tcp 13.52.84.161:6443: connect: connection refused
Mar 04 20:50:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:50:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:50:23.913 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-3" is terminated: "Error" - "egator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 20:50:20.712865       1 server.go:692] external host was not specified, using 10.0.133.9\nI0304 20:50:20.713307       1 server.go:152] Version: v1.12.4+761b685\nF0304 20:50:20.713474       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 20:50:32.395 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-2 container stopped being ready
Mar 04 20:50:33.299 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-2" is terminated: "Error" - "_builder.go:124: Failed to list <nil>: Get https://localhost:6443/apis/operatorstatus.openshift.io/v1/clusteroperators?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nI0304 20:50:32.059261       1 stateful_set.go:163] Shutting down statefulset controller\nI0304 20:50:32.059267       1 pv_controller_base.go:287] Shutting down persistent volume controller\nI0304 20:50:32.060329       1 pv_controller_base.go:398] claim worker queue shutting down\nI0304 20:50:32.059272       1 gc_controller.go:86] Shutting down GC controller\nI0304 20:50:32.059277       1 resource_quota_controller.go:297] Shutting down resource quota controller\nI0304 20:50:32.059282       1 attach_detach_controller.go:341] Shutting down attach detach controller\nI0304 20:50:32.059290       1 service_controller.go:197] Shutting down service controller\nI0304 20:50:32.059296       1 expand_controller.go:165] Shutting down expand controller\nI0304 20:50:32.059300       1 endpoints_controller.go:166] Shutting down endpoint controller\nI0304 20:50:32.059305       1 replica_set.go:194] Shutting down replicationcontroller controller\nI0304 20:50:32.059312       1 horizontal.go:167] Shutting down HPA controller\nI0304 20:50:32.059317       1 clusterroleaggregation_controller.go:160] Shutting down ClusterRoleAggregator\nI0304 20:50:32.059322       1 deployment_controller.go:164] Shutting down deployment controller\nI0304 20:50:32.059327       1 node_ipam_controller.go:180] Shutting down ipam controller\nI0304 20:50:32.059364       1 node_lifecycle_controller.go:430] Shutting down node controller\nI0304 20:50:32.059374       1 replica_set.go:194] Shutting down replicaset controller\nI0304 20:50:32.059379       1 disruption.go:305] Shutting down disruption controller\nI0304 20:50:32.059384       1 pvc_protection_controller.go:111] Shutting down PVC protection controller\nI0304 20:50:32.059616       1 certificate_controller.go:125] Shutting down certificate controller\nI0304 20:50:32.059627       1 job_controller.go:155] Shutting down job controller\n"
Mar 04 20:50:37.409 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-3 container restarted
Mar 04 20:50:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:50:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:50:39.512 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-3" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:50:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:50:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:51:03.101 I ns/openshift-kube-apiserver pod/installer-3-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:51:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:51:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:51:14.280 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:51:14.300 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:51:14.304 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:51:14.324 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:51:15.075 W ns/openshift-kube-apiserver pod/installer-3-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:51:17.093 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-3 container restarted
Mar 04 20:51:19.492 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-2 container restarted
Mar 04 20:51:19.525 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:51:20.109 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-3" is waiting: "CrashLoopBackOff" - "Back-off 10s restarting failed container=kube-apiserver-3 pod=kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal_openshift-kube-apiserver(c3014f657c7fb301bab799b7f5ce6a65)"
Mar 04 20:51:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:51:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:51:28.124 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-controller-manager-2 container stopped being ready
Mar 04 20:51:28.136 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container stopped being ready
Mar 04 20:51:28.157 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal container="kube-controller-manager-2" is terminated: "Error" - "\nI0304 20:51:27.723329       1 job_controller.go:155] Shutting down job controller\nI0304 20:51:27.723330       1 pv_controller_base.go:287] Shutting down persistent volume controller\nI0304 20:51:27.723342       1 pv_controller_base.go:398] claim worker queue shutting down\nI0304 20:51:27.723205       1 pvc_protection_controller.go:111] Shutting down PVC protection controller\nI0304 20:51:27.723370       1 stateful_set.go:163] Shutting down statefulset controller\nI0304 20:51:27.723391       1 attach_detach_controller.go:341] Shutting down attach detach controller\nI0304 20:51:27.723439       1 graph_builder.go:336] stopped 69 of 69 monitors\nI0304 20:51:27.723455       1 graph_builder.go:337] GraphBuilder stopping\nI0304 20:51:27.723476       1 resource_quota_controller.go:266] resource quota controller worker shutting down\nI0304 20:51:27.723486       1 resource_quota_controller.go:266] resource quota controller worker shutting down\nI0304 20:51:27.723497       1 resource_quota_controller.go:266] resource quota controller worker shutting down\nI0304 20:51:27.723517       1 resource_quota_controller.go:266] resource quota controller worker shutting down\nI0304 20:51:27.723528       1 resource_quota_controller.go:266] resource quota controller worker shutting down\nI0304 20:51:27.723539       1 pv_controller_base.go:341] volume worker queue shutting down\nI0304 20:51:27.723706       1 cronjob_controller.go:98] Shutting down CronJob Manager\nI0304 20:51:27.723727       1 cleaner.go:89] Shutting down CSR cleaner controller\nI0304 20:51:27.723742       1 node_lifecycle_controller.go:430] Shutting down node controller\nI0304 20:51:27.723759       1 service_controller.go:197] Shutting down service controller\nI0304 20:51:27.723772       1 horizontal.go:167] Shutting down HPA controller\nI0304 20:51:27.723822       1 horizontal.go:200] horizontal pod autoscaler controller worker shutting down\nI0304 20:51:27.723856       1 tokens_controller.go:189] Shutting down\nF0304 20:51:27.723936       1 controllermanager.go:258] leaderelection lost\n"
Mar 04 20:51:29.138 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-controller-manager-2 container restarted
Mar 04 20:51:29.148 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:51:30.745 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:51:32.152 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-3 container restarted
Mar 04 20:51:34.910 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-3" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:51:37.134 E clusteroperator/kube-apiserver changed Failing to True: NodeInstallerFailing: NodeInstallerFailing: 0 nodes are failing on revision 3:\nNodeInstallerFailing: static pod has been installed, but is not ready while new revision is pending
Mar 04 20:51:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:51:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:51:41.496 I ns/openshift-kube-apiserver pod/installer-4-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:51:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:51:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:51:52.973 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:51:52.998 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:51:53.006 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:51:53.018 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:51:54.209 W ns/openshift-kube-apiserver pod/installer-4-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:51:55.208 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-4 container restarted
Mar 04 20:52:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:52:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:52:12.321 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-4 container restarted
Mar 04 20:52:13.898 E clusterversion/version changed Failing to True: ClusterOperatorFailing: Cluster operator kube-apiserver is reporting a failure: NodeInstallerFailing: 0 nodes are failing on revision 3:\nNodeInstallerFailing: \nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-4" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-4" is waiting: "CrashLoopBackOff" - "Back-off 10s restarting failed container=kube-apiserver-4 pod=kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal_openshift-kube-apiserver(565d60208cc9f2784f4b296b6cdf1c03)"
Mar 04 20:52:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:52:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:52:28.106 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:52:35.497 I ns/openshift-kube-apiserver pod/revision-pruner-4-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:52:35.692 I ns/openshift-kube-apiserver pod/installer-4-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:52:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:52:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:52:43.900 W clusterversion/version changed Failing to False
Mar 04 20:52:47.479 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:52:47.501 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:52:47.509 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:52:47.519 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:52:48.664 W ns/openshift-kube-apiserver pod/installer-4-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:52:49.672 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-4 container restarted
Mar 04 20:52:52.507 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-4" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-4" is waiting: "CrashLoopBackOff" - "Back-off 10s restarting failed container=kube-apiserver-4 pod=kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal_openshift-kube-apiserver(565d60208cc9f2784f4b296b6cdf1c03)"
Mar 04 20:52:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:52:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:53:02.711 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container stopped being ready
Mar 04 20:53:03.601 E clusteroperator/kube-scheduler changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal container="scheduler" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal container="scheduler" is terminated: "Error" - "nformers/factory.go:131: Failed to list *v1beta1.PodDisruptionBudget: Get https://localhost:6443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:01.621509       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.StorageClass: Get https://localhost:6443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:01.622613       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.PersistentVolumeClaim: Get https://localhost:6443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:01.623680       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.StatefulSet: Get https://localhost:6443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:01.809854       1 event.go:259] Could not construct reference to: '&v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:\"\"}, Subsets:[]v1.EndpointSubset(nil)}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'LeaderElection' 'ip-10-0-133-9_6079ced7-3ebe-11e9-a53e-068e62be1050 stopped leading'\nI0304 20:53:01.809934       1 leaderelection.go:249] failed to renew lease kube-system/kube-scheduler: failed to tryAcquireOrRenew context deadline exceeded\nE0304 20:53:01.809950       1 server.go:207] lost master\nlost lease\n"
Mar 04 20:53:03.716 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:53:06.198 W clusteroperator/kube-scheduler changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:53:06.726 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-4 container restarted
Mar 04 20:53:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:53:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:53:09.705 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-4" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:53:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:53:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:53:25.696 I ns/openshift-kube-apiserver pod/installer-4-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:53:37.185 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:53:37.209 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:53:37.217 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:53:37.220 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:53:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:53:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:53:37.968 W ns/openshift-kube-apiserver pod/installer-4-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:53:39.973 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-4 container restarted
Mar 04 20:53:41.536 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-4" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-4" is terminated: "Error" - "gator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 20:53:39.326168       1 server.go:692] external host was not specified, using 10.0.136.28\nI0304 20:53:39.326590       1 server.go:152] Version: v1.12.4+761b685\nF0304 20:53:39.326734       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 20:53:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:53:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:53:53.006 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-2 container stopped being ready
Mar 04 20:53:53.043 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-2" is terminated: "Error" - "ection refused\nE0304 20:53:52.107655       1 reflector.go:125] k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:124: Failed to list <nil>: Get https://localhost:6443/apis/config.openshift.io/v1/features?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:52.151752       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1beta1.PodSecurityPolicy: Get https://localhost:6443/apis/policy/v1beta1/podsecuritypolicies?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:52.157620       1 reflector.go:125] k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:124: Failed to list <nil>: Get https://localhost:6443/apis/config.openshift.io/v1/images?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 20:53:52.159006       1 event.go:259] Could not construct reference to: '&v1.ConfigMap{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:\"\"}, Data:map[string]string(nil), BinaryData:map[string][]uint8(nil)}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'LeaderElection' 'ip-10-0-136-28_e4b9abb9-3ebe-11e9-b3fe-0651195654be stopped leading'\nI0304 20:53:52.159071       1 leaderelection.go:249] failed to renew lease kube-system/kube-controller-manager: failed to tryAcquireOrRenew context deadline exceeded\nF0304 20:53:52.159128       1 controllermanager.go:258] leaderelection lost\nI0304 20:53:52.203418       1 deployment_controller.go:164] Shutting down deployment controller\n"
Mar 04 20:53:54.012 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-2 container restarted
Mar 04 20:53:55.428 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:53:57.024 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-4 container restarted
Mar 04 20:53:59.132 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-4" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:54:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:54:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:54:09.106 W clusteroperator/kube-apiserver changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 4
Mar 04 20:54:12.942 I ns/openshift-machine-config-operator pod/machine-config-operator-64cff49768-4psq8 node/ created
Mar 04 20:54:13.071 I ns/openshift-cloud-credential-operator pod/cloud-credential-operator-f4864d9f8-xh7cm node/ created
Mar 04 20:54:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:54:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:54:27.058 I clusteroperator/openshift-cloud-credential-operator created
Mar 04 20:54:27.084 E clusteroperator/openshift-cloud-credential-operator changed Failing to True: CredentialsFailing: 1 of 2 credentials requests are failing to sync.
Mar 04 20:54:28.189 I ns/openshift-machine-api pod/machine-api-operator-57c45f78c5-qrgp5 node/ created
Mar 04 20:54:30.113 W clusteroperator/openshift-cloud-credential-operator changed Failing to False: NoCredentialsFailing: No credentials requests reporting errors.
Mar 04 20:54:30.113 W clusteroperator/openshift-cloud-credential-operator changed Progressing to False: ReconcilingComplete: 2 of 2 credentials requests provisioned and reconciled.
Mar 04 20:54:31.844 I clusteroperator/machine-config created
Mar 04 20:54:31.859 W clusteroperator/machine-config changed Progressing to True: Working towards 4.0.0-alpha.0-5-gf87d8b8d-dirty
Mar 04 20:54:32.050 I ns/openshift-machine-config-operator pod/machine-config-controller-59f746f64c-wqs5v node/ created
Mar 04 20:54:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:54:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:54:42.357 I clusteroperator/machine-api created
Mar 04 20:54:42.414 I ns/openshift-machine-api pod/clusterapi-manager-controllers-b995f45f9-z67pv node/ created
Mar 04 20:54:47.175 I ns/openshift-machine-config-operator pod/machine-config-server-m7zwd node/ created
Mar 04 20:54:47.182 I ns/openshift-machine-config-operator pod/machine-config-server-x2lsw node/ created
Mar 04 20:54:47.193 I ns/openshift-machine-config-operator pod/machine-config-server-2tkf6 node/ created
Mar 04 20:54:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:54:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:54:53.256 I ns/openshift-machine-config-operator pod/machine-config-daemon-bswbw node/ created
Mar 04 20:54:53.268 I ns/openshift-machine-config-operator pod/machine-config-daemon-29bmz node/ created
Mar 04 20:54:53.268 I ns/openshift-machine-config-operator pod/machine-config-daemon-rgss9 node/ created
Mar 04 20:55:00.269 W clusteroperator/machine-config changed Available to True: Cluster has deployed 4.0.0-alpha.0-5-gf87d8b8d-dirty
Mar 04 20:55:00.450 W clusteroperator/machine-config changed Progressing to False: Cluster version is 4.0.0-alpha.0-5-gf87d8b8d-dirty
Mar 04 20:55:04.396 W clusteroperator/machine-api changed Progressing to False
Mar 04 20:55:07.500 I ns/openshift-machine-api pod/cluster-autoscaler-operator-dc5777cc9-xmvgf node/ created
Mar 04 20:55:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:55:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:55:09.342 I ns/openshift-operator-lifecycle-manager pod/olm-operator-684cbd686c-gpkmt node/ created
Mar 04 20:55:09.345 I ns/openshift-operator-lifecycle-manager pod/catalog-operator-778d56bffd-n5tbd node/ created
Mar 04 20:55:14.852 E kube-apiserver Kube API started failing: Get https://api.sjenning.devcluster.openshift.com:6443/api/v1/namespaces/kube-system?timeout=3s: context deadline exceeded (Client.Timeout exceeded while awaiting headers)
Mar 04 20:55:18.172 I kube-apiserver Kube API started responding to GET requests
Mar 04 20:55:22.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:55:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:55:25.065 I clusteroperator/cluster-autoscaler created
Mar 04 20:55:26.043 I ns/openshift-operator-lifecycle-manager pod/olm-operators-5295f node/ created
Mar 04 20:55:26.065 I clusteroperator/operator-lifecycle-manager created
Mar 04 20:55:27.415 I ns/openshift-apiserver-operator pod/openshift-apiserver-operator-86cb4cddd4-mwx6x node/ created
Mar 04 20:55:37.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:55:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:55:41.865 I clusteroperator/openshift-apiserver created
Mar 04 20:55:42.425 W clusteroperator/openshift-apiserver changed Upgradeable to True
Mar 04 20:55:43.425 W clusteroperator/openshift-apiserver changed Failing to False
Mar 04 20:55:44.443 E clusteroperator/openshift-apiserver changed Failing to True: ResourceSyncControllerFailing: ResourceSyncControllerFailing: namespaces "openshift-apiserver" not found\nResourceSyncControllerFailing: namespaces "openshift-apiserver" not found
Mar 04 20:55:45.245 W clusteroperator/openshift-apiserver changed Failing to False
Mar 04 20:55:46.873 I ns/openshift-apiserver pod/apiserver-qs8l9 node/ created
Mar 04 20:55:46.883 I ns/openshift-apiserver pod/apiserver-4vws6 node/ created
Mar 04 20:55:46.885 I ns/openshift-apiserver pod/apiserver-npggj node/ created
Mar 04 20:55:49.054 W clusteroperator/openshift-apiserver changed Progressing to True: Progressing: Progressing: daemonset/apiserver.openshift-operator: observed generation is 1, desired generation is 2.\nProgressing: openshiftapiserveroperatorconfigs/instance: observed generation is 0, desired generation is 2.
Mar 04 20:55:49.054 W clusteroperator/openshift-apiserver changed Available to False: Available: Available: no openshift-apiserver daemon pods available on any node.
Mar 04 20:55:49.055 W ns/openshift-apiserver pod/apiserver-npggj node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 70s
Mar 04 20:55:49.058 W ns/openshift-apiserver pod/apiserver-qs8l9 node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 70s
Mar 04 20:55:49.059 W ns/openshift-apiserver pod/apiserver-4vws6 node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 70s
Mar 04 20:55:51.247 W clusteroperator/openshift-apiserver changed Progressing to False
Mar 04 20:55:51.538 I ns/openshift-operator-lifecycle-manager pod/packageserver-5f85549784-qgdkx node/ created
Mar 04 20:55:51.548 I ns/openshift-operator-lifecycle-manager pod/packageserver-5f85549784-599dp node/ created
Mar 04 20:55:52.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:55:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:55:53.213 W ns/openshift-apiserver pod/apiserver-qs8l9 node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:55:53.218 W ns/openshift-apiserver pod/apiserver-4vws6 node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:55:53.238 W ns/openshift-apiserver pod/apiserver-npggj node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:55:53.248 I ns/openshift-apiserver pod/apiserver-8d4jb node/ created
Mar 04 20:55:53.265 I ns/openshift-apiserver pod/apiserver-gr6tg node/ created
Mar 04 20:55:53.266 I ns/openshift-apiserver pod/apiserver-mvthb node/ created
Mar 04 20:56:07.852 E openshift-apiserver OpenShift API is not responding to GET requests
Mar 04 20:56:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:56:08.914 I openshift-apiserver OpenShift API started responding to GET requests
Mar 04 20:56:14.518 W clusteroperator/openshift-apiserver changed Available to True
Mar 04 20:56:16.559 I ns/openshift-controller-manager-operator pod/openshift-controller-manager-operator-5c95d8fc89-9z86h node/ created
Mar 04 20:56:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:56:31.312 I clusteroperator/openshift-controller-manager created
Mar 04 20:56:31.420 W clusteroperator/openshift-controller-manager changed Failing to False
Mar 04 20:56:33.506 I ns/openshift-controller-manager pod/controller-manager-272p7 node/ created
Mar 04 20:56:33.523 I ns/openshift-controller-manager pod/controller-manager-vp45w node/ created
Mar 04 20:56:33.523 I ns/openshift-controller-manager pod/controller-manager-tlfq6 node/ created
Mar 04 20:56:35.371 W ns/openshift-controller-manager pod/controller-manager-tlfq6 node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:56:35.379 W ns/openshift-controller-manager pod/controller-manager-272p7 node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:56:35.380 W ns/openshift-controller-manager pod/controller-manager-vp45w node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:56:35.384 W clusteroperator/openshift-controller-manager changed Progressing to True: Progressing: Progressing: daemonset/controller-manager: observed generation is 1, desired generation is 2.
Mar 04 20:56:35.384 W clusteroperator/openshift-controller-manager changed Available to False: Available: Available: no daemon pods available on any node.
Mar 04 20:56:37.354 W clusteroperator/openshift-controller-manager changed Progressing to False
Mar 04 20:56:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:56:39.857 W ns/openshift-controller-manager pod/controller-manager-272p7 node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:56:39.860 W ns/openshift-controller-manager pod/controller-manager-tlfq6 node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:56:39.898 W ns/openshift-controller-manager pod/controller-manager-vp45w node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:56:39.912 I ns/openshift-controller-manager pod/controller-manager-9rgt7 node/ created
Mar 04 20:56:39.915 I ns/openshift-controller-manager pod/controller-manager-cgk9c node/ created
Mar 04 20:56:39.918 I ns/openshift-controller-manager pod/controller-manager-4kz4s node/ created
Mar 04 20:56:51.459 W clusteroperator/openshift-controller-manager changed Available to True
Mar 04 20:56:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:56:52.952 W clusteroperator/openshift-cloud-credential-operator changed Progressing to True: Reconciling: 2 of 3 credentials requests provisioned, 0 reporting errors.
Mar 04 20:56:52.971 E clusteroperator/openshift-cloud-credential-operator changed Failing to True: CredentialsFailing: 1 of 3 credentials requests are failing to sync.
Mar 04 20:56:54.285 W clusteroperator/openshift-cloud-credential-operator changed Failing to False: NoCredentialsFailing: No credentials requests reporting errors.
Mar 04 20:56:55.346 W clusteroperator/openshift-cloud-credential-operator changed Progressing to False: ReconcilingComplete: 4 of 4 credentials requests provisioned and reconciled.
Mar 04 20:57:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:57:10.307 I ns/openshift-monitoring pod/cluster-monitoring-operator-798f74f95-z5p6s node/ created
Mar 04 20:57:10.317 I ns/openshift-cluster-storage-operator pod/cluster-storage-operator-75b7d9fdbc-bgcfr node/ created
Mar 04 20:57:14.353 I ns/openshift-cluster-node-tuning-operator pod/cluster-node-tuning-operator-787c7f99f4-s54jr node/ created
Mar 04 20:57:15.393 I ns/openshift-cluster-samples-operator pod/cluster-samples-operator-85b467679f-8h78x node/ created
Mar 04 20:57:20.454 I ns/openshift-image-registry pod/cluster-image-registry-operator-6c97bfc564-ncmk4 node/ created
Mar 04 20:57:21.373 I ns/openshift-svcat-apiserver-operator pod/openshift-svcat-apiserver-operator-6984bd87c6-csflr node/ created
Mar 04 20:57:21.424 I ns/openshift-authentication-operator pod/openshift-authentication-operator-554c764d8c-sfzbd node/ created
Mar 04 20:57:21.828 I ns/openshift-ingress-operator pod/ingress-operator-946657f95-s958n node/ created
Mar 04 20:57:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:57:23.374 I ns/openshift-svcat-controller-manager-operator pod/openshift-svcat-controller-manager-operator-68d7cfd6d8-kvndn node/ created
Mar 04 20:57:23.591 I clusteroperator/monitoring created
Mar 04 20:57:24.180 I clusteroperator/storage created
Mar 04 20:57:25.495 I ns/openshift-console-operator pod/console-operator-6fd65465b8-slqq5 node/ created
Mar 04 20:57:25.533 I ns/openshift-marketplace pod/marketplace-operator-868d98c5c6-kw9n8 node/ created
Mar 04 20:57:28.739 I ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ created
Mar 04 20:57:28.954 I ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ created
Mar 04 20:57:29.461 I clusteroperator/node-tuning created
Mar 04 20:57:29.753 I ns/openshift-monitoring pod/node-exporter-g5qwt node/ created
Mar 04 20:57:29.820 I ns/openshift-monitoring pod/node-exporter-mwdzb node/ created
Mar 04 20:57:29.823 I ns/openshift-monitoring pod/node-exporter-zl8nt node/ created
Mar 04 20:57:29.946 I ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ created
Mar 04 20:57:29.981 I ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ created
Mar 04 20:57:30.505 I ns/openshift-cluster-node-tuning-operator pod/tuned-94s5f node/ created
Mar 04 20:57:30.547 I ns/openshift-cluster-node-tuning-operator pod/tuned-x78ps node/ created
Mar 04 20:57:30.550 I ns/openshift-cluster-node-tuning-operator pod/tuned-tws5k node/ created
Mar 04 20:57:30.613 I ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ created
Mar 04 20:57:30.690 W clusteroperator/node-tuning changed Available to False: DaemonSet "tuned" has 3 unavailable pod(s).
Mar 04 20:57:30.690 W clusteroperator/node-tuning changed Progressing to True
Mar 04 20:57:31.796 I clusteroperator/openshift-samples created
Mar 04 20:57:33.822 I clusteroperator/image-registry created
Mar 04 20:57:34.877 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:34.877 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:34.908 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:34.908 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:34.908 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:34.972 W clusteroperator/openshift-samples changed Progressing to True: Samples installation in error at 4.0.0-alpha1-709a49010: invalid configuration
Mar 04 20:57:36.852 I clusteroperator/openshift-authentication created
Mar 04 20:57:36.983 W clusteroperator/openshift-authentication changed Failing to False
Mar 04 20:57:37.187 E clusteroperator/openshift-authentication changed Failing to True: Failing: Failing: secrets "v4-0-config-system-router-certs" not found
Mar 04 20:57:37.187 W clusteroperator/openshift-authentication changed Progressing to False
Mar 04 20:57:37.187 W clusteroperator/openshift-authentication changed Available to False
Mar 04 20:57:37.390 I ns/openshift-image-registry pod/node-ca-spmbr node/ created
Mar 04 20:57:37.443 I ns/openshift-image-registry pod/image-registry-6c646779f6-kmqqh node/ created
Mar 04 20:57:37.537 I ns/openshift-image-registry pod/node-ca-fqp68 node/ created
Mar 04 20:57:37.544 I ns/openshift-image-registry pod/node-ca-2lgj4 node/ created
Mar 04 20:57:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:57:37.974 W ns/openshift-image-registry pod/node-ca-spmbr node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:57:37.975 W ns/openshift-image-registry pod/node-ca-2lgj4 node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:57:37.978 W ns/openshift-image-registry pod/node-ca-fqp68 node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:57:38.385 I ns/openshift-image-registry pod/image-registry-5f7549cdfd-f26pf node/ created
Mar 04 20:57:39.257 I ns/openshift-ingress pod/router-default-68676766d9-qqft9 node/ created
Mar 04 20:57:39.285 I ns/openshift-ingress pod/router-default-68676766d9-kjsnw node/ created
Mar 04 20:57:39.621 W ns/openshift-apiserver pod/apiserver-gr6tg node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 70s
Mar 04 20:57:39.715 W clusteroperator/node-tuning changed Available to True
Mar 04 20:57:39.715 W clusteroperator/node-tuning changed Progressing to False
Mar 04 20:57:40.343 I clusteroperator/ingress created
Mar 04 20:57:41.211 W clusteroperator/openshift-apiserver changed Progressing to True: Progressing: Progressing: daemonset/apiserver.openshift-operator: observed generation is 2, desired generation is 3.\nProgressing: openshiftapiserveroperatorconfigs/instance: observed generation is 2, desired generation is 3.
Mar 04 20:57:41.827 W ns/openshift-apiserver pod/apiserver-gr6tg node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated
Mar 04 20:57:41.827 W ns/openshift-apiserver pod/apiserver-gr6tg node/ip-10-0-136-28.us-west-1.compute.internal container=openshift-apiserver container stopped being ready
Mar 04 20:57:42.045 I clusteroperator/marketplace-operator created
Mar 04 20:57:42.954 W ns/openshift-apiserver pod/apiserver-gr6tg node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:57:42.977 I ns/openshift-apiserver pod/apiserver-bz76d node/ created
Mar 04 20:57:43.334 I clusteroperator/console created
Mar 04 20:57:43.435 W clusteroperator/openshift-apiserver changed Progressing to False
Mar 04 20:57:43.631 E clusteroperator/console changed Failing to True: Failing: Failing: "secret": secret not found, creating new secret, create error = <nil>\nFailing: 
Mar 04 20:57:43.631 W clusteroperator/console changed Progressing to True: Progressing: Progressing: "secret": secret not found, creating new secret, create error = <nil>\nProgressing: 
Mar 04 20:57:43.631 W clusteroperator/console changed Available to True: AsExpected: Available: "secret": secret not found, creating new secret, create error = <nil>\nAvailable: 
Mar 04 20:57:43.825 W ns/openshift-image-registry pod/node-ca-spmbr node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:57:43.940 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:43.940 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:43.940 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:43.950 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:43.950 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:43.950 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:44.394 W ns/openshift-image-registry pod/node-ca-fqp68 node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:57:44.522 W clusteroperator/console changed Failing to False
Mar 04 20:57:44.522 W clusteroperator/console changed Available to False: Available: Available: No pods available for console deployment.
Mar 04 20:57:44.540 I ns/openshift-console pod/console-655f4dd87b-db246 node/ created
Mar 04 20:57:44.569 I ns/openshift-console pod/console-655f4dd87b-r6fh5 node/ created
Mar 04 20:57:44.969 I ns/openshift-marketplace pod/certified-operators-75fb56bdc8-v5rfl node/ created
Mar 04 20:57:45.039 W clusteroperator/openshift-authentication changed Failing to False
Mar 04 20:57:45.039 W clusteroperator/openshift-authentication changed Available to True
Mar 04 20:57:45.042 W ns/openshift-image-registry pod/node-ca-2lgj4 node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:57:45.099 I ns/openshift-authentication pod/openshift-authentication-6487d585c4-mph7n node/ created
Mar 04 20:57:45.137 I ns/openshift-image-registry pod/node-ca-h7t7n node/ created
Mar 04 20:57:45.199 I ns/openshift-image-registry pod/node-ca-96kxk node/ created
Mar 04 20:57:45.200 I ns/openshift-image-registry pod/node-ca-zw4g8 node/ created
Mar 04 20:57:45.263 I ns/openshift-marketplace pod/community-operators-cf87b4958-fl95k node/ created
Mar 04 20:57:45.615 I ns/openshift-marketplace pod/redhat-operators-86496d9f6d-khtj9 node/ created
Mar 04 20:57:47.215 I ns/openshift-authentication pod/openshift-authentication-d6f6f95b4-dr25s node/ created
Mar 04 20:57:47.824 W clusteroperator/kube-controller-manager changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 2
Mar 04 20:57:49.217 I ns/openshift-kube-controller-manager pod/revision-pruner-3-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:57:49.399 I ns/openshift-kube-controller-manager pod/installer-3-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:57:50.146 W clusteroperator/kube-scheduler changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 1
Mar 04 20:57:51.641 W clusteroperator/openshift-controller-manager changed Progressing to True: Progressing: Progressing: daemonset/controller-manager: observed generation is 2, desired generation is 3.
Mar 04 20:57:51.642 W ns/openshift-controller-manager pod/controller-manager-4kz4s node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:57:52.308 I ns/openshift-kube-scheduler pod/installer-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:57:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:57:55.245 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:55.245 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:55.245 W clusteroperator/openshift-samples changed Failing to False
Mar 04 20:57:55.245 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:55.259 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:55.259 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:55.259 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:57.642 W clusteroperator/image-registry changed Available to True: Deployment has minimum availability
Mar 04 20:57:57.890 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:57:57.929 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:57:57.966 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:57:58.028 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:57:58.299 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:58.299 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:58.299 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 20:57:58.313 W clusteroperator/openshift-samples changed Available to True: Samples installation successful at 4.0.0-alpha1-709a49010
Mar 04 20:57:58.313 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:58.313 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:58.313 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 20:57:58.370 W ns/openshift-image-registry pod/image-registry-6c646779f6-kmqqh node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:57:58.448 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is waiting: "ContainerCreating" - ""
Mar 04 20:57:58.471 W clusteroperator/image-registry changed Progressing to False: Everything is ready
Mar 04 20:57:59.878 W ns/openshift-authentication pod/openshift-authentication-6487d585c4-mph7n node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:58:00.446 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:58:00.711 W ns/openshift-authentication pod/openshift-authentication-6487d585c4-mph7n node/ip-10-0-156-96.us-west-1.compute.internal container=openshift-authentication container stopped being ready
Mar 04 20:58:01.123 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:58:01.123 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:58:01.123 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container stopped being ready
Mar 04 20:58:01.178 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:58:01.191 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:58:01.200 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:58:01.429 W ns/openshift-apiserver pod/apiserver-mvthb node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 70s
Mar 04 20:58:01.687 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 20:58:02.641 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is terminated: "Error" - "       1 flags.go:33] FLAG: --secondary-node-eviction-rate=\"0.01\"\nI0304 20:57:58.876797       1 flags.go:33] FLAG: --secure-port=\"10257\"\nI0304 20:57:58.876835       1 flags.go:33] FLAG: --service-account-private-key-file=\"\"\nI0304 20:57:58.876871       1 flags.go:33] FLAG: --service-cluster-ip-range=\"\"\nI0304 20:57:58.876908       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 20:57:58.876945       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 20:57:58.876983       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 20:57:58.877020       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 20:57:58.877057       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 20:57:58.877095       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 20:57:58.877137       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 20:57:58.877174       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 20:57:58.877212       1 flags.go:33] FLAG: --terminated-pod-gc-threshold=\"12500\"\nI0304 20:57:58.877253       1 flags.go:33] FLAG: --tls-cert-file=\"\"\nI0304 20:57:58.877291       1 flags.go:33] FLAG: --tls-cipher-suites=\"[]\"\nI0304 20:57:58.877344       1 flags.go:33] FLAG: --tls-min-version=\"\"\nI0304 20:57:58.877381       1 flags.go:33] FLAG: --tls-private-key-file=\"\"\nI0304 20:57:58.877416       1 flags.go:33] FLAG: --tls-sni-cert-key=\"[]\"\nI0304 20:57:58.877457       1 flags.go:33] FLAG: --unhealthy-zone-threshold=\"0.55\"\nI0304 20:57:58.877496       1 flags.go:33] FLAG: --use-service-account-credentials=\"false\"\nI0304 20:57:58.877548       1 flags.go:33] FLAG: --v=\"2\"\nI0304 20:57:58.877596       1 flags.go:33] FLAG: --version=\"false\"\nI0304 20:57:58.877647       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 20:57:59.326060       1 serving.go:293] Generated self-signed cert (/var/run/kubernetes/kube-controller-manager.crt, /var/run/kubernetes/kube-controller-manager.key)\nfailed to create listener: failed to listen on 0.0.0.0:10252: listen tcp 0.0.0.0:10252: bind: address already in use\n"
Mar 04 20:58:02.746 W ns/openshift-apiserver pod/apiserver-mvthb node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated
Mar 04 20:58:02.746 W ns/openshift-apiserver pod/apiserver-mvthb node/ip-10-0-133-9.us-west-1.compute.internal container=openshift-apiserver container stopped being ready
Mar 04 20:58:03.713 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:58:05.516 E clusteroperator/kube-scheduler changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal container="scheduler" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal container="scheduler" is terminated: "Error" - "--log-dir=\"\"\nI0304 20:58:03.065428       1 flags.go:33] FLAG: --log-flush-frequency=\"5s\"\nI0304 20:58:03.065434       1 flags.go:33] FLAG: --logtostderr=\"true\"\nI0304 20:58:03.065440       1 flags.go:33] FLAG: --machine-id-file=\"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0304 20:58:03.065449       1 flags.go:33] FLAG: --master=\"\"\nI0304 20:58:03.065455       1 flags.go:33] FLAG: --mesos-agent=\"127.0.0.1:5051\"\nI0304 20:58:03.065461       1 flags.go:33] FLAG: --mesos-agent-timeout=\"10s\"\nI0304 20:58:03.065468       1 flags.go:33] FLAG: --policy-config-file=\"\"\nI0304 20:58:03.065474       1 flags.go:33] FLAG: --policy-configmap=\"\"\nI0304 20:58:03.065480       1 flags.go:33] FLAG: --policy-configmap-namespace=\"kube-system\"\nI0304 20:58:03.065487       1 flags.go:33] FLAG: --port=\"10251\"\nI0304 20:58:03.065496       1 flags.go:33] FLAG: --profiling=\"false\"\nI0304 20:58:03.065503       1 flags.go:33] FLAG: --scheduler-name=\"default-scheduler\"\nI0304 20:58:03.065510       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 20:58:03.065516       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 20:58:03.065523       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 20:58:03.065530       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 20:58:03.065576       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 20:58:03.065583       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 20:58:03.065590       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 20:58:03.065596       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 20:58:03.065603       1 flags.go:33] FLAG: --use-legacy-policy-config=\"false\"\nI0304 20:58:03.065610       1 flags.go:33] FLAG: --v=\"2\"\nI0304 20:58:03.065617       1 flags.go:33] FLAG: --version=\"false\"\nI0304 20:58:03.065640       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 20:58:03.065649       1 flags.go:33] FLAG: --write-config-to=\"\"\nfailed to create listener: failed to listen on 0.0.0.0:10251: listen tcp 0.0.0.0:10251: bind: address already in use\n"
Mar 04 20:58:06.450 W ns/openshift-authentication pod/openshift-authentication-6487d585c4-mph7n node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:58:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:58:07.852 W ns/openshift-apiserver pod/apiserver-mvthb node/ip-10-0-133-9.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:11.852 I openshift-apiserver OpenShift API started failing: Get https://api.sjenning.devcluster.openshift.com:6443/apis/image.openshift.io/v1/namespaces/openshift-apiserver/imagestreams/missing?timeout=3s: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
Mar 04 20:58:11.853 E kube-apiserver Kube API started failing: Get https://api.sjenning.devcluster.openshift.com:6443/api/v1/namespaces/kube-system?timeout=3s: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
Mar 04 20:58:15.310 I kube-apiserver Kube API started responding to GET requests
Mar 04 20:58:17.337 I openshift-apiserver OpenShift API started responding to GET requests
Mar 04 20:58:17.443 W ns/openshift-apiserver pod/apiserver-mvthb node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:58:17.493 I ns/openshift-apiserver pod/apiserver-tmkrm node/ created
Mar 04 20:58:17.514 W clusteroperator/openshift-controller-manager changed Progressing to False
Mar 04 20:58:19.805 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:58:19.826 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 20:58:20.162 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:58:22.825 W ns/openshift-controller-manager pod/controller-manager-4kz4s node/ip-10-0-133-9.us-west-1.compute.internal container=controller-manager container stopped being ready
Mar 04 20:58:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:58:23.776 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is terminated: "Error" - "       1 flags.go:33] FLAG: --secondary-node-eviction-rate=\"0.01\"\nI0304 20:58:18.925584       1 flags.go:33] FLAG: --secure-port=\"10257\"\nI0304 20:58:18.925589       1 flags.go:33] FLAG: --service-account-private-key-file=\"\"\nI0304 20:58:18.925593       1 flags.go:33] FLAG: --service-cluster-ip-range=\"\"\nI0304 20:58:18.925597       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 20:58:18.925601       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 20:58:18.925606       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 20:58:18.925610       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 20:58:18.925614       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 20:58:18.925618       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 20:58:18.925622       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 20:58:18.925626       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 20:58:18.925630       1 flags.go:33] FLAG: --terminated-pod-gc-threshold=\"12500\"\nI0304 20:58:18.925634       1 flags.go:33] FLAG: --tls-cert-file=\"\"\nI0304 20:58:18.925638       1 flags.go:33] FLAG: --tls-cipher-suites=\"[]\"\nI0304 20:58:18.925646       1 flags.go:33] FLAG: --tls-min-version=\"\"\nI0304 20:58:18.925650       1 flags.go:33] FLAG: --tls-private-key-file=\"\"\nI0304 20:58:18.925654       1 flags.go:33] FLAG: --tls-sni-cert-key=\"[]\"\nI0304 20:58:18.925659       1 flags.go:33] FLAG: --unhealthy-zone-threshold=\"0.55\"\nI0304 20:58:18.925664       1 flags.go:33] FLAG: --use-service-account-credentials=\"false\"\nI0304 20:58:18.925668       1 flags.go:33] FLAG: --v=\"2\"\nI0304 20:58:18.925672       1 flags.go:33] FLAG: --version=\"false\"\nI0304 20:58:18.925678       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 20:58:19.857989       1 serving.go:293] Generated self-signed cert (/var/run/kubernetes/kube-controller-manager.crt, /var/run/kubernetes/kube-controller-manager.key)\nfailed to create listener: failed to listen on 0.0.0.0:10252: listen tcp 0.0.0.0:10252: bind: address already in use\n"
Mar 04 20:58:25.645 I ns/openshift-machine-config-operator pod/machine-config-daemon-w6nh2 node/ created
Mar 04 20:58:25.707 I ns/openshift-sdn pod/sdn-tx2tm node/ created
Mar 04 20:58:25.732 I ns/openshift-image-registry pod/node-ca-cgg49 node/ created
Mar 04 20:58:25.757 W clusteroperator/network changed Progressing to True: Deploying: DaemonSet "openshift-sdn/sdn" is not available (awaiting 1 nodes)
Mar 04 20:58:25.757 W clusteroperator/network changed Available to False: Deploying: DaemonSet "openshift-sdn/sdn" is not available (awaiting 1 nodes)
Mar 04 20:58:25.758 I ns/openshift-multus pod/multus-5qd28 node/ created
Mar 04 20:58:25.824 I ns/openshift-cluster-node-tuning-operator pod/tuned-bxrfg node/ created
Mar 04 20:58:25.850 I ns/openshift-sdn pod/ovs-g5mx8 node/ created
Mar 04 20:58:25.863 I ns/openshift-monitoring pod/node-exporter-6bcgb node/ created
Mar 04 20:58:25.903 I ns/openshift-dns pod/dns-default-z96rh node/ created
Mar 04 20:58:25.990 W clusteroperator/node-tuning changed Available to False: DaemonSet "tuned" has 1 unavailable pod(s).
Mar 04 20:58:25.990 W clusteroperator/node-tuning changed Progressing to True
Mar 04 20:58:26.130 I ns/openshift-machine-config-operator pod/machine-config-daemon-c5wt9 node/ created
Mar 04 20:58:26.167 I ns/openshift-sdn pod/sdn-7dpxs node/ created
Mar 04 20:58:26.175 I ns/openshift-image-registry pod/node-ca-sqwds node/ created
Mar 04 20:58:26.197 I ns/openshift-multus pod/multus-rrvk4 node/ created
Mar 04 20:58:26.231 I ns/openshift-cluster-node-tuning-operator pod/tuned-s87vq node/ created
Mar 04 20:58:26.253 I ns/openshift-monitoring pod/node-exporter-mfs7v node/ created
Mar 04 20:58:26.317 I ns/openshift-sdn pod/ovs-8qd2s node/ created
Mar 04 20:58:26.518 I ns/openshift-dns pod/dns-default-xm68k node/ created
Mar 04 20:58:29.017 W ns/openshift-image-registry pod/image-registry-6c646779f6-kmqqh node/ip-10-0-136-28.us-west-1.compute.internal container=registry container stopped being ready
Mar 04 20:58:35.509 W ns/openshift-controller-manager pod/controller-manager-4kz4s node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:58:35.536 I ns/openshift-controller-manager pod/controller-manager-ksfsf node/ created
Mar 04 20:58:36.877 W ns/openshift-image-registry pod/image-registry-6c646779f6-kmqqh node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:58:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:58:37.852 W node/ip-10-0-145-63.us-west-1.compute.internal node is not ready
Mar 04 20:58:37.852 W node/ip-10-0-135-169.us-west-1.compute.internal node is not ready
Mar 04 20:58:37.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:37.852 W ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:37.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:37.852 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:37.852 W ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:40.092 W ns/openshift-apiserver pod/apiserver-8d4jb node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 70s
Mar 04 20:58:40.794 I ns/openshift-cluster-node-tuning-operator pod/tuned-xj6dd node/ created
Mar 04 20:58:40.803 W ns/openshift-apiserver pod/apiserver-8d4jb node/ip-10-0-156-96.us-west-1.compute.internal container=openshift-apiserver container stopped being ready
Mar 04 20:58:40.820 I ns/openshift-sdn pod/ovs-t44bf node/ created
Mar 04 20:58:40.828 I ns/openshift-monitoring pod/node-exporter-2r2jg node/ created
Mar 04 20:58:40.849 I ns/openshift-dns pod/dns-default-ch7p6 node/ created
Mar 04 20:58:40.860 I ns/openshift-multus pod/multus-ltmgq node/ created
Mar 04 20:58:40.929 I ns/openshift-sdn pod/sdn-cdrxf node/ created
Mar 04 20:58:44.922 W ns/openshift-controller-manager pod/controller-manager-9rgt7 node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:58:45.916 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:58:46.451 W ns/openshift-apiserver pod/apiserver-8d4jb node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:58:46.473 I ns/openshift-apiserver pod/apiserver-k66jw node/ created
Mar 04 20:58:47.188 W clusteroperator/kube-scheduler changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:58:48.928 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 20:58:48.964 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-133-9.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:58:52.350 I ns/openshift-kube-scheduler pod/revision-pruner-2-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:58:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:58:52.852 W node/ip-10-0-143-101.us-west-1.compute.internal node is not ready
Mar 04 20:58:52.852 W node/ip-10-0-135-169.us-west-1.compute.internal node is not ready
Mar 04 20:58:52.852 W node/ip-10-0-145-63.us-west-1.compute.internal node is not ready
Mar 04 20:58:52.852 W ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-marketplace pod/redhat-operators-86496d9f6d-khtj9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-marketplace pod/certified-operators-75fb56bdc8-v5rfl node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-marketplace pod/community-operators-cf87b4958-fl95k node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-ingress pod/router-default-68676766d9-kjsnw node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-ingress pod/router-default-68676766d9-qqft9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:52.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:58:56.344 I ns/openshift-kube-scheduler pod/installer-2-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:58:56.966 W clusteroperator/kube-apiserver changed Progressing to True: Progressing: Progressing: 3 nodes are at revision 4
Mar 04 20:58:58.944 I ns/openshift-kube-apiserver pod/revision-pruner-5-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:58:59.141 I ns/openshift-kube-apiserver pod/installer-5-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:59:04.451 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:59:04.451 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:59:04.451 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container stopped being ready
Mar 04 20:59:04.486 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:59:04.496 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:59:04.510 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:59:05.275 I ns/openshift-kube-controller-manager pod/installer-3-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:59:06.882 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:59:07.261 W node/ip-10-0-135-169.us-west-1.compute.internal condition Ready changed
Mar 04 20:59:07.579 E clusteroperator/kube-scheduler changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal container="scheduler" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal container="scheduler" is terminated: "Error" - "--log-dir=\"\"\nI0304 20:59:06.201952       1 flags.go:33] FLAG: --log-flush-frequency=\"5s\"\nI0304 20:59:06.201956       1 flags.go:33] FLAG: --logtostderr=\"true\"\nI0304 20:59:06.201960       1 flags.go:33] FLAG: --machine-id-file=\"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0304 20:59:06.201965       1 flags.go:33] FLAG: --master=\"\"\nI0304 20:59:06.201970       1 flags.go:33] FLAG: --mesos-agent=\"127.0.0.1:5051\"\nI0304 20:59:06.201974       1 flags.go:33] FLAG: --mesos-agent-timeout=\"10s\"\nI0304 20:59:06.201978       1 flags.go:33] FLAG: --policy-config-file=\"\"\nI0304 20:59:06.201982       1 flags.go:33] FLAG: --policy-configmap=\"\"\nI0304 20:59:06.201986       1 flags.go:33] FLAG: --policy-configmap-namespace=\"kube-system\"\nI0304 20:59:06.201990       1 flags.go:33] FLAG: --port=\"10251\"\nI0304 20:59:06.201996       1 flags.go:33] FLAG: --profiling=\"false\"\nI0304 20:59:06.202000       1 flags.go:33] FLAG: --scheduler-name=\"default-scheduler\"\nI0304 20:59:06.202005       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 20:59:06.202009       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 20:59:06.202014       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 20:59:06.202018       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 20:59:06.202022       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 20:59:06.202026       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 20:59:06.202030       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 20:59:06.202034       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 20:59:06.202038       1 flags.go:33] FLAG: --use-legacy-policy-config=\"false\"\nI0304 20:59:06.202042       1 flags.go:33] FLAG: --v=\"2\"\nI0304 20:59:06.202046       1 flags.go:33] FLAG: --version=\"false\"\nI0304 20:59:06.202053       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 20:59:06.202057       1 flags.go:33] FLAG: --write-config-to=\"\"\nfailed to create listener: failed to listen on 0.0.0.0:10251: listen tcp 0.0.0.0:10251: bind: address already in use\n"
Mar 04 20:59:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:59:07.852 W node/ip-10-0-143-101.us-west-1.compute.internal node is not ready
Mar 04 20:59:07.852 W node/ip-10-0-145-63.us-west-1.compute.internal node is not ready
Mar 04 20:59:07.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-ingress pod/router-default-68676766d9-kjsnw node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-ingress pod/router-default-68676766d9-qqft9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-marketplace pod/certified-operators-75fb56bdc8-v5rfl node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-marketplace pod/redhat-operators-86496d9f6d-khtj9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-marketplace pod/community-operators-cf87b4958-fl95k node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:07.852 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:11.140 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:59:11.166 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:59:11.179 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 20:59:11.193 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 20:59:12.014 W ns/openshift-kube-apiserver pod/installer-5-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 20:59:13.482 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 20:59:13.494 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 0s
Mar 04 20:59:13.505 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 20:59:13.525 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 20:59:13.538 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is waiting: "ContainerCreating" - ""
Mar 04 20:59:14.031 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-5 container restarted
Mar 04 20:59:15.387 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:59:15.984 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-5" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-5" is terminated: "Error" - "egator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 20:59:13.384433       1 server.go:692] external host was not specified, using 10.0.133.9\nI0304 20:59:13.384949       1 server.go:152] Version: v1.12.4+761b685\nF0304 20:59:13.385157       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 20:59:16.954 W node/ip-10-0-145-63.us-west-1.compute.internal condition Ready changed
Mar 04 20:59:17.347 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is terminated: "Error" - "       1 flags.go:33] FLAG: --secondary-node-eviction-rate=\"0.01\"\nI0304 20:59:14.446916       1 flags.go:33] FLAG: --secure-port=\"10257\"\nI0304 20:59:14.446923       1 flags.go:33] FLAG: --service-account-private-key-file=\"\"\nI0304 20:59:14.446930       1 flags.go:33] FLAG: --service-cluster-ip-range=\"\"\nI0304 20:59:14.446937       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 20:59:14.446944       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 20:59:14.446951       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 20:59:14.446958       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 20:59:14.446966       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 20:59:14.446972       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 20:59:14.446979       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 20:59:14.446986       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 20:59:14.446993       1 flags.go:33] FLAG: --terminated-pod-gc-threshold=\"12500\"\nI0304 20:59:14.447000       1 flags.go:33] FLAG: --tls-cert-file=\"\"\nI0304 20:59:14.447007       1 flags.go:33] FLAG: --tls-cipher-suites=\"[]\"\nI0304 20:59:14.447014       1 flags.go:33] FLAG: --tls-min-version=\"\"\nI0304 20:59:14.447021       1 flags.go:33] FLAG: --tls-private-key-file=\"\"\nI0304 20:59:14.447028       1 flags.go:33] FLAG: --tls-sni-cert-key=\"[]\"\nI0304 20:59:14.447037       1 flags.go:33] FLAG: --unhealthy-zone-threshold=\"0.55\"\nI0304 20:59:14.447045       1 flags.go:33] FLAG: --use-service-account-credentials=\"false\"\nI0304 20:59:14.447052       1 flags.go:33] FLAG: --v=\"2\"\nI0304 20:59:14.447059       1 flags.go:33] FLAG: --version=\"false\"\nI0304 20:59:14.447069       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 20:59:14.778378       1 serving.go:293] Generated self-signed cert (/var/run/kubernetes/kube-controller-manager.crt, /var/run/kubernetes/kube-controller-manager.key)\nfailed to create listener: failed to listen on 0.0.0.0:10252: listen tcp 0.0.0.0:10252: bind: address already in use\n"
Mar 04 20:59:20.086 W clusteroperator/node-tuning changed Available to True
Mar 04 20:59:20.086 W clusteroperator/node-tuning changed Progressing to False
Mar 04 20:59:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:59:22.852 W node/ip-10-0-143-101.us-west-1.compute.internal node is not ready
Mar 04 20:59:22.852 W ns/openshift-marketplace pod/community-operators-cf87b4958-fl95k node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-ingress pod/router-default-68676766d9-kjsnw node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-marketplace pod/certified-operators-75fb56bdc8-v5rfl node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-marketplace pod/redhat-operators-86496d9f6d-khtj9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:22.852 W ns/openshift-ingress pod/router-default-68676766d9-qqft9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:24.468 W ns/openshift-controller-manager pod/controller-manager-9rgt7 node/ip-10-0-156-96.us-west-1.compute.internal container=controller-manager container stopped being ready
Mar 04 20:59:24.680 W ns/openshift-controller-manager pod/controller-manager-9rgt7 node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 20:59:24.699 I ns/openshift-controller-manager pod/controller-manager-nrhmf node/ created
Mar 04 20:59:25.923 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 20:59:26.067 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:59:31.007 W node/ip-10-0-143-101.us-west-1.compute.internal condition Ready changed
Mar 04 20:59:31.065 I ns/openshift-machine-config-operator pod/machine-config-daemon-nwp4f node/ created
Mar 04 20:59:31.100 I ns/openshift-image-registry pod/node-ca-ft77t node/ created
Mar 04 20:59:34.081 W ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs node/ip-10-0-133-9.us-west-1.compute.internal container=cluster-version-operator container stopped being ready
Mar 04 20:59:34.190 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 20:59:34.766 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:59:34.980 W ns/openshift-controller-manager pod/controller-manager-cgk9c node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 30s
Mar 04 20:59:35.081 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-5 container restarted
Mar 04 20:59:36.350 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is terminated: "Error" - "       1 flags.go:33] FLAG: --secondary-node-eviction-rate=\"0.01\"\nI0304 20:59:34.201388       1 flags.go:33] FLAG: --secure-port=\"10257\"\nI0304 20:59:34.201395       1 flags.go:33] FLAG: --service-account-private-key-file=\"\"\nI0304 20:59:34.201402       1 flags.go:33] FLAG: --service-cluster-ip-range=\"\"\nI0304 20:59:34.201409       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 20:59:34.201416       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 20:59:34.201423       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 20:59:34.201430       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 20:59:34.201438       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 20:59:34.201444       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 20:59:34.201451       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 20:59:34.201458       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 20:59:34.201465       1 flags.go:33] FLAG: --terminated-pod-gc-threshold=\"12500\"\nI0304 20:59:34.201472       1 flags.go:33] FLAG: --tls-cert-file=\"\"\nI0304 20:59:34.201479       1 flags.go:33] FLAG: --tls-cipher-suites=\"[]\"\nI0304 20:59:34.201488       1 flags.go:33] FLAG: --tls-min-version=\"\"\nI0304 20:59:34.201494       1 flags.go:33] FLAG: --tls-private-key-file=\"\"\nI0304 20:59:34.201501       1 flags.go:33] FLAG: --tls-sni-cert-key=\"[]\"\nI0304 20:59:34.201511       1 flags.go:33] FLAG: --unhealthy-zone-threshold=\"0.55\"\nI0304 20:59:34.201519       1 flags.go:33] FLAG: --use-service-account-credentials=\"false\"\nI0304 20:59:34.201526       1 flags.go:33] FLAG: --v=\"2\"\nI0304 20:59:34.201534       1 flags.go:33] FLAG: --version=\"false\"\nI0304 20:59:34.201544       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 20:59:34.727666       1 serving.go:293] Generated self-signed cert (/var/run/kubernetes/kube-controller-manager.crt, /var/run/kubernetes/kube-controller-manager.key)\nfailed to create listener: failed to listen on 0.0.0.0:10252: listen tcp 0.0.0.0:10252: bind: address already in use\n"
Mar 04 20:59:37.092 W ns/openshift-cluster-version pod/cluster-version-operator-5dcf9fb898-9jrhs node/ip-10-0-133-9.us-west-1.compute.internal container=cluster-version-operator container restarted
Mar 04 20:59:37.159 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-5" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:59:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:59:37.852 W ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-ingress pod/router-default-68676766d9-kjsnw node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-image-registry pod/node-ca-cgg49 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-multus pod/multus-5qd28 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-dns pod/dns-default-z96rh node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-marketplace pod/community-operators-cf87b4958-fl95k node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-ingress pod/router-default-68676766d9-qqft9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-marketplace pod/certified-operators-75fb56bdc8-v5rfl node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-marketplace pod/redhat-operators-86496d9f6d-khtj9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:37.852 W ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:49.024 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=scheduler container restarted
Mar 04 20:59:49.962 W clusteroperator/kube-scheduler changed Failing to False: AsExpected: MonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 20:59:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 20:59:52.852 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-monitoring pod/kube-state-metrics-76f9c947f4-vqp44 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-2zmwq node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-ingress pod/router-default-68676766d9-qqft9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-marketplace pod/certified-operators-75fb56bdc8-v5rfl node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-marketplace pod/redhat-operators-86496d9f6d-khtj9 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-dns pod/dns-default-z96rh node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-monitoring pod/telemeter-client-54d4d4f659-w7tdp node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-marketplace pod/community-operators-cf87b4958-fl95k node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-ingress pod/router-default-68676766d9-kjsnw node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-image-registry pod/node-ca-cgg49 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-multus pod/multus-5qd28 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:52.852 W ns/openshift-monitoring pod/prometheus-adapter-fc6b8f6-s94n7 node/ip-10-0-145-63.us-west-1.compute.internal pod has been pending longer than a minute
Mar 04 20:59:53.272 W clusteroperator/network changed Progressing to False
Mar 04 20:59:53.272 W clusteroperator/network changed Available to True
Mar 04 20:59:54.506 I ns/openshift-authentication pod/openshift-authentication-6446f578cd-f5mgr node/ created
Mar 04 20:59:56.182 I ns/openshift-kube-scheduler pod/revision-pruner-2-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 20:59:58.174 I ns/openshift-monitoring pod/grafana-5cfd56f478-2lvrt node/ created
Mar 04 20:59:58.942 I ns/openshift-kube-scheduler pod/installer-2-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 21:00:02.550 I ns/openshift-kube-apiserver pod/installer-5-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:00:05.104 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:05.104 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:05.104 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:05.119 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 21:00:05.119 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 21:00:05.119 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 21:00:05.130 W clusteroperator/openshift-samples changed Progressing to False: Samples installation successful at 
Mar 04 21:00:05.667 W ns/openshift-controller-manager pod/controller-manager-cgk9c node/ip-10-0-136-28.us-west-1.compute.internal container=controller-manager container stopped being ready
Mar 04 21:00:06.617 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:00:06.617 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=scheduler container stopped being ready
Mar 04 21:00:06.649 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:00:06.661 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 21:00:06.674 I ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 21:00:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:00:08.151 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:08.151 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:08.151 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:08.167 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 21:00:08.167 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 21:00:08.167 I clusteroperator/openshift-samples versions: operator 4.0.0-alpha1-709a49010 -> 
Mar 04 21:00:08.693 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=scheduler container restarted
Mar 04 21:00:08.768 E clusteroperator/kube-scheduler changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal container="scheduler" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal container="scheduler" is terminated: "Error" - "--log-dir=\"\"\nI0304 21:00:08.005212       1 flags.go:33] FLAG: --log-flush-frequency=\"5s\"\nI0304 21:00:08.005216       1 flags.go:33] FLAG: --logtostderr=\"true\"\nI0304 21:00:08.005220       1 flags.go:33] FLAG: --machine-id-file=\"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0304 21:00:08.005227       1 flags.go:33] FLAG: --master=\"\"\nI0304 21:00:08.005234       1 flags.go:33] FLAG: --mesos-agent=\"127.0.0.1:5051\"\nI0304 21:00:08.005241       1 flags.go:33] FLAG: --mesos-agent-timeout=\"10s\"\nI0304 21:00:08.005248       1 flags.go:33] FLAG: --policy-config-file=\"\"\nI0304 21:00:08.005254       1 flags.go:33] FLAG: --policy-configmap=\"\"\nI0304 21:00:08.005261       1 flags.go:33] FLAG: --policy-configmap-namespace=\"kube-system\"\nI0304 21:00:08.005268       1 flags.go:33] FLAG: --port=\"10251\"\nI0304 21:00:08.005275       1 flags.go:33] FLAG: --profiling=\"false\"\nI0304 21:00:08.005280       1 flags.go:33] FLAG: --scheduler-name=\"default-scheduler\"\nI0304 21:00:08.005284       1 flags.go:33] FLAG: --stderrthreshold=\"2\"\nI0304 21:00:08.005288       1 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\"\nI0304 21:00:08.005293       1 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\"\nI0304 21:00:08.005297       1 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\"\nI0304 21:00:08.005301       1 flags.go:33] FLAG: --storage-driver-password=\"root\"\nI0304 21:00:08.005388       1 flags.go:33] FLAG: --storage-driver-secure=\"false\"\nI0304 21:00:08.005397       1 flags.go:33] FLAG: --storage-driver-table=\"stats\"\nI0304 21:00:08.005404       1 flags.go:33] FLAG: --storage-driver-user=\"root\"\nI0304 21:00:08.005411       1 flags.go:33] FLAG: --use-legacy-policy-config=\"false\"\nI0304 21:00:08.005417       1 flags.go:33] FLAG: --v=\"2\"\nI0304 21:00:08.005424       1 flags.go:33] FLAG: --version=\"false\"\nI0304 21:00:08.005435       1 flags.go:33] FLAG: --vmodule=\"\"\nI0304 21:00:08.005443       1 flags.go:33] FLAG: --write-config-to=\"\"\nfailed to create listener: failed to listen on 0.0.0.0:10251: listen tcp 0.0.0.0:10251: bind: address already in use\n"
Mar 04 21:00:09.715 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 21:00:09.760 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nMonitoringResourceControllerFailing: the server could not find the requested resource
Mar 04 21:00:10.727 W ns/openshift-authentication pod/openshift-authentication-d6f6f95b4-dr25s node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 30s
Mar 04 21:00:11.205 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:11.205 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:11.205 I clusteroperator/openshift-samples versions: operator  -> 4.0.0-alpha1-709a49010
Mar 04 21:00:11.709 W ns/openshift-authentication pod/openshift-authentication-d6f6f95b4-dr25s node/ip-10-0-136-28.us-west-1.compute.internal container=openshift-authentication container stopped being ready
Mar 04 21:00:12.700 W clusteroperator/ingress changed Available to True
Mar 04 21:00:13.912 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:00:13.930 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:00:13.944 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 21:00:13.963 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:00:15.084 W ns/openshift-kube-apiserver pod/installer-5-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 21:00:16.095 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-5 container restarted
Mar 04 21:00:16.152 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-5" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal container="kube-apiserver-5" is terminated: "Error" - "gator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 21:00:14.882611       1 server.go:692] external host was not specified, using 10.0.156.96\nI0304 21:00:14.883030       1 server.go:152] Version: v1.12.4+761b685\nF0304 21:00:14.883200       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 21:00:16.867 W ns/openshift-controller-manager pod/controller-manager-cgk9c node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 21:00:16.891 I ns/openshift-controller-manager pod/controller-manager-mcgr9 node/ created
Mar 04 21:00:16.909 W ns/openshift-authentication pod/openshift-authentication-d6f6f95b4-dr25s node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 21:00:22.853 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:00:24.743 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=scheduler container restarted
Mar 04 21:00:28.133 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-controller-manager-2 container stopped being ready
Mar 04 21:00:28.272 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal container="kube-controller-manager-2" is not ready\nStaticPodsFailing: nodes/ip-10-0-156-96.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal container="kube-controller-manager-2" is terminated: "Error" - " https://localhost:6443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:00:27.262078       1 reflector.go:125] k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:124: Failed to list <nil>: Get https://localhost:6443/apis/monitoring.coreos.com/v1/prometheuses?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:00:27.298136       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.DaemonSet: Get https://localhost:6443/apis/apps/v1/daemonsets?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:00:27.312009       1 reflector.go:125] k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:124: Failed to list <nil>: Get https://localhost:6443/apis/operator.openshift.io/v1/servicecas?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:00:27.334807       1 event.go:259] Could not construct reference to: '&v1.ConfigMap{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:\"\"}, Data:map[string]string(nil), BinaryData:map[string][]uint8(nil)}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'LeaderElection' 'ip-10-0-156-96_49972094-3ebf-11e9-98e9-0229a0f5e3f0 stopped leading'\nI0304 21:00:27.334883       1 leaderelection.go:249] failed to renew lease kube-system/kube-controller-manager: failed to tryAcquireOrRenew context deadline exceeded\nF0304 21:00:27.334965       1 controllermanager.go:258] leaderelection lost\n"
Mar 04 21:00:32.062 I ns/openshift-kube-controller-manager pod/installer-3-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:00:32.152 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-5 container restarted
Mar 04 21:00:33.744 I ns/openshift-kube-apiserver pod/installer-6-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:00:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:00:40.248 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:00:40.273 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:00:40.277 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 21:00:40.293 I ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:00:41.084 W clusteroperator/kube-controller-manager changed Failing to False
Mar 04 21:00:41.181 W ns/openshift-kube-controller-manager pod/installer-3-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 21:00:45.779 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:00:45.796 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:00:45.799 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 21:00:45.809 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:00:46.281 W ns/openshift-kube-apiserver pod/installer-6-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 21:00:48.284 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-6 container restarted
Mar 04 21:00:48.344 W ns/openshift-console pod/console-655f4dd87b-db246 node/ip-10-0-133-9.us-west-1.compute.internal container=console container restarted
Mar 04 21:00:49.889 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=scheduler container restarted
Mar 04 21:00:51.412 W clusteroperator/kube-scheduler changed Failing to False
Mar 04 21:00:51.606 W clusteroperator/kube-scheduler changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 2
Mar 04 21:00:52.111 W clusteroperator/monitoring changed Available to False
Mar 04 21:00:52.111 W clusteroperator/monitoring changed Progressing to False
Mar 04 21:00:52.111 E clusteroperator/monitoring changed Failing to True: Failed to rollout the stack. Error: running task Updating configuration sharing failed: failed to retrieve Prometheus host: getting Route object failed: routes.route.openshift.io "prometheus-k8s" not found
Mar 04 21:00:52.120 W clusteroperator/monitoring changed Progressing to True: Rolling out the stack.
Mar 04 21:00:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:00:53.266 W ns/openshift-console pod/console-655f4dd87b-r6fh5 node/ip-10-0-156-96.us-west-1.compute.internal container=console container restarted
Mar 04 21:00:56.356 I ns/openshift-monitoring pod/prometheus-operator-599f4df856-vpn6d node/ created
Mar 04 21:00:58.143 I ns/openshift-kube-scheduler pod/revision-pruner-2-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 21:01:00.432 W clusteroperator/kube-controller-manager changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 3
Mar 04 21:01:01.082 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created
Mar 04 21:01:02.292 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-6 container restarted
Mar 04 21:01:03.542 I ns/openshift-kube-apiserver pod/installer-7-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:01:05.829 I ns/openshift-monitoring pod/prometheus-k8s-0 node/ created
Mar 04 21:01:05.921 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created
Mar 04 21:01:06.879 W ns/openshift-kube-scheduler pod/revision-pruner-2-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=pruner container stopped being ready
Mar 04 21:01:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:01:10.036 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal graceful deletion within 30s
Mar 04 21:01:11.404 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated
Mar 04 21:01:11.404 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal container=prometheus-operator container stopped being ready
Mar 04 21:01:15.110 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:01:15.154 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:01:15.163 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal deleted
Mar 04 21:01:15.177 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal created
Mar 04 21:01:15.334 W ns/openshift-kube-apiserver pod/installer-7-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 21:01:18.357 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-7 container restarted
Mar 04 21:01:22.841 W ns/openshift-monitoring pod/prometheus-operator-664445f8fd-g9zjv node/ip-10-0-145-63.us-west-1.compute.internal deleted
Mar 04 21:01:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:01:24.113 I ns/openshift-monitoring pod/alertmanager-main-1 node/ created
Mar 04 21:01:32.458 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-156-96.us-west-1.compute.internal node/ip-10-0-156-96.us-west-1.compute.internal container=kube-apiserver-7 container restarted
Mar 04 21:01:35.937 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-143-101.us-west-1.compute.internal container=prometheus container restarted
Mar 04 21:01:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:01:38.203 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-145-63.us-west-1.compute.internal container=prometheus container restarted
Mar 04 21:01:39.248 I ns/openshift-monitoring pod/alertmanager-main-2 node/ created
Mar 04 21:01:50.552 W clusteroperator/kube-apiserver changed Failing to False
Mar 04 21:01:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:02:00.965 W clusteroperator/monitoring changed Available to True: Successfully rolled out the stack.
Mar 04 21:02:00.965 W clusteroperator/monitoring changed Progressing to False
Mar 04 21:02:00.965 W clusteroperator/monitoring changed Failing to False
Mar 04 21:02:01.343 I ns/openshift-kube-apiserver pod/installer-7-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 21:02:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:02:12.989 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:02:13.012 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:02:13.022 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal deleted
Mar 04 21:02:13.031 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal created
Mar 04 21:02:14.066 W ns/openshift-kube-apiserver pod/installer-7-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 21:02:14.184 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-7" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-7" is terminated: "Error" - "gator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 21:02:13.945522       1 server.go:692] external host was not specified, using 10.0.136.28\nI0304 21:02:13.945942       1 server.go:152] Version: v1.12.4+761b685\nF0304 21:02:13.946108       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 21:02:15.067 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-7 container restarted
Mar 04 21:02:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:02:28.114 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-3 container stopped being ready
Mar 04 21:02:28.149 E clusteroperator/kube-controller-manager changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready\nStaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is terminated: "Error" - "to list *v1.EgressNetworkPolicy: Get https://localhost:6443/apis/network.openshift.io/v1/egressnetworkpolicies?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:02:27.633562       1 reflector.go:125] github.com/openshift/client-go/build/informers/externalversions/factory.go:101: Failed to list *v1.Build: Get https://localhost:6443/apis/build.openshift.io/v1/builds?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:02:27.638003       1 reflector.go:125] k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:124: Failed to list <nil>: Get https://localhost:6443/apis/machineconfiguration.openshift.io/v1/kubeletconfigs?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:02:27.649487       1 cronjob_controller.go:109] can't list Jobs: Get https://localhost:6443/apis/batch/v1/jobs: dial tcp [::1]:6443: connect: connection refused\nE0304 21:02:27.652849       1 event.go:259] Could not construct reference to: '&v1.ConfigMap{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:\"\"}, Data:map[string]string(nil), BinaryData:map[string][]uint8(nil)}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'LeaderElection' 'ip-10-0-136-28_8028de17-3ec0-11e9-a1b4-0651195654be stopped leading'\nI0304 21:02:27.652915       1 leaderelection.go:249] failed to renew lease kube-system/kube-controller-manager: failed to tryAcquireOrRenew context deadline exceeded\nF0304 21:02:27.652986       1 controllermanager.go:258] leaderelection lost\n"
Mar 04 21:02:33.130 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-apiserver-7 container restarted
Mar 04 21:02:35.162 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-136-28.us-west-1.compute.internal container="kube-apiserver-7" is not ready
Mar 04 21:02:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:02:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:02:58.789 I ns/openshift-kube-apiserver pod/revision-pruner-7-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 21:02:58.940 I ns/openshift-kube-apiserver pod/installer-7-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 21:03:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:03:09.959 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal invariant violation (bug): static pod should not transition Running->Pending with same UID
Mar 04 21:03:09.984 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal graceful deletion within 0s
Mar 04 21:03:09.994 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal deleted
Mar 04 21:03:10.008 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal created
Mar 04 21:03:10.693 W ns/openshift-kube-apiserver pod/installer-7-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=installer container stopped being ready
Mar 04 21:03:12.249 W ns/openshift-kube-controller-manager pod/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal node/ip-10-0-136-28.us-west-1.compute.internal container=kube-controller-manager-3 container restarted
Mar 04 21:03:12.285 W clusteroperator/kube-controller-manager changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-136-28.us-west-1.compute.internal pods/kube-controller-manager-ip-10-0-136-28.us-west-1.compute.internal container="kube-controller-manager-3" is not ready
Mar 04 21:03:12.710 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-7 container restarted
Mar 04 21:03:14.364 E clusteroperator/kube-apiserver changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-7" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-7" is terminated: "Error" - "egator-client/tls.key --requestheader-allowed-names=kube-apiserver-proxy --requestheader-allowed-names=system:kube-apiserver-proxy --requestheader-allowed-names=system:openshift-aggregator --requestheader-client-ca-file=/etc/kubernetes/static-pod-resources/configmaps/aggregator-client-ca/ca-bundle.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-cluster-ip-range=172.30.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --storage-media-type=application/vnd.kubernetes.protobuf --tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-cipher-suites=TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA --tls-cipher-suites=TLS_RSA_WITH_AES_256_CBC_SHA --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key]`\nI0304 21:03:12.128888       1 server.go:692] external host was not specified, using 10.0.133.9\nI0304 21:03:12.129369       1 server.go:152] Version: v1.12.4+761b685\nF0304 21:03:12.129586       1 cmd.go:71] failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n"
Mar 04 21:03:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:03:24.758 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container stopped being ready
Mar 04 21:03:26.192 E clusteroperator/kube-scheduler changed Failing to True: StaticPodsFailing: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal container="scheduler" is not ready\nStaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal container="scheduler" is terminated: "Error" - "factory.go:131: Failed to list *v1.ReplicationController: Get https://localhost:6443/api/v1/replicationcontrollers?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:03:23.387065       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.Service: Get https://localhost:6443/api/v1/services?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:03:23.388198       1 reflector.go:125] k8s.io/client-go/informers/factory.go:131: Failed to list *v1.PersistentVolume: Get https://localhost:6443/api/v1/persistentvolumes?limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:03:23.389238       1 reflector.go:125] k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:178: Failed to list *v1.Pod: Get https://localhost:6443/api/v1/pods?fieldSelector=status.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&limit=500&resourceVersion=0: dial tcp [::1]:6443: connect: connection refused\nE0304 21:03:23.821154       1 event.go:259] Could not construct reference to: '&v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:\"\"}, Subsets:[]v1.EndpointSubset(nil)}' due to: 'selfLink was empty, can't make reference'. Will not report event: 'Normal' 'LeaderElection' 'ip-10-0-133-9_4cfee7b1-3ec0-11e9-9049-068e62be1050 stopped leading'\nI0304 21:03:23.821237       1 leaderelection.go:249] failed to renew lease kube-system/kube-scheduler: failed to tryAcquireOrRenew context deadline exceeded\nE0304 21:03:23.821255       1 server.go:207] lost master\nlost lease\n"
Mar 04 21:03:30.778 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=kube-apiserver-7 container restarted
Mar 04 21:03:32.958 W clusteroperator/kube-apiserver changed Failing to False: AsExpected: StaticPodsFailing: nodes/ip-10-0-133-9.us-west-1.compute.internal pods/kube-apiserver-ip-10-0-133-9.us-west-1.compute.internal container="kube-apiserver-7" is not ready
Mar 04 21:03:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:03:38.817 W ns/openshift-console pod/console-655f4dd87b-db246 node/ip-10-0-133-9.us-west-1.compute.internal container=console container restarted
Mar 04 21:03:42.830 W ns/openshift-console pod/console-655f4dd87b-r6fh5 node/ip-10-0-156-96.us-west-1.compute.internal container=console container restarted
Mar 04 21:03:48.041 W clusteroperator/console changed Progressing to False
Mar 04 21:03:48.041 W clusteroperator/console changed Available to True
Mar 04 21:03:50.087 W clusteroperator/kube-apiserver changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 7
Mar 04 21:03:52.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:04:07.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:04:09.902 W ns/openshift-kube-scheduler pod/openshift-kube-scheduler-ip-10-0-133-9.us-west-1.compute.internal node/ip-10-0-133-9.us-west-1.compute.internal container=scheduler container restarted
Mar 04 21:04:11.359 W clusteroperator/kube-scheduler changed Failing to False
Mar 04 21:04:22.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:04:37.852 W clusterversion/version cluster is updating to 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:04:42.988 W clusterversion/version cluster reached 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:04:42.988 W clusterversion/version changed Available to True: Done applying 4.0.0-0.alpha-2019-03-04-190542
Mar 04 21:04:42.988 W clusterversion/version changed Progressing to False: Cluster version is 4.0.0-0.alpha-2019-03-04-190542
